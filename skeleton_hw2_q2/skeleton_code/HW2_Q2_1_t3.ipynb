{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbXSD0GH3RfN"
   },
   "source": [
    "# Deep Learning Homework 2 - Question 2.1\n",
    "## RNA Binding Protein (RBP) Interaction Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ma8O0mu3RfO"
   },
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ao6tSGgx3RfO"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openpyxl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QjVtMaVQ3RfP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Create Output Directory\n",
    "OUTPUT_DIR = 'Outputs_t3'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRH8WJjw3RfP"
   },
   "source": [
    "## 2. Download Data\n",
    "\n",
    "Download the data files from the Google Drive link provided in the homework:\n",
    "- `norm_data.txt`\n",
    "- `metadata.xlsx`\n",
    "\n",
    "Upload them to Colab or mount your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6zMi0qRK5OrF"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kC91wjUR3RfP"
   },
   "outputs": [],
   "source": [
    "# Set the path to your data files\n",
    "DATA_DIR = 'data'  # Change this to your folder path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bWdIDwo3RfP"
   },
   "source": [
    "## 3. Configuration and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dEKieqTx3RfP"
   },
   "outputs": [],
   "source": [
    "from config import RNAConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5ebeuKmQ3RfQ"
   },
   "outputs": [],
   "source": [
    "from utils import configure_seed, masked_mse_loss, masked_spearman_correlation\n",
    "\n",
    "configure_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8kY3uRe3RfQ"
   },
   "source": [
    "## 4. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n1mTI2lD3RfQ"
   },
   "outputs": [],
   "source": [
    "from utils import RNACompeteLoader, load_rnacompete_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LIA0znn3RfQ"
   },
   "source": [
    "## 5. Model Definitions\n",
    "\n",
    "### 5.1 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HG1UcssY3RfQ"
   },
   "outputs": [],
   "source": [
    "class RNABindingCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    1D Convolutional Neural Network for RNA sequence binding prediction.\n",
    "\n",
    "    Architecture:\n",
    "    - 3 convolutional layers with increasing channels (64 -> 128 -> 256)\n",
    "    - Batch normalization after each conv layer\n",
    "    - ReLU activation and dropout for regularization\n",
    "    - Global max + average pooling for richer representation\n",
    "    - 2 fully connected layers for regression output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels=4, seq_length=41, hidden_dim=128, dropout=0.3):\n",
    "        super(RNABindingCNN, self).__init__()\n",
    "        #L_out = ((L_in + 2P - K) / S)  + 1\n",
    "\n",
    "        # Convolutional layers with different kernel sizes to capture various motif lengths\n",
    "        # Padding choices allow to preserve input length through the feature maps\n",
    "        #L_out = 41\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        #L_out = 41\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=7, padding=3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        #L_out = 41\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=9, padding=4)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Global pooling (both max and average)\n",
    "        # allows to get the strongest value of a given feature for the sequence\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        # allows to get the average of how much a feature is present throughout the sequence\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 2, hidden_dim)  # *2 for concat of max and avg pool\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch, seq_length, 4)\n",
    "        # Conv1d expects: (batch, channels, seq_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        max_pool = self.global_max_pool(x).squeeze(-1)\n",
    "        avg_pool = self.global_avg_pool(x).squeeze(-1)\n",
    "        x = torch.cat([max_pool, avg_pool], dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBoXj9sR3RfQ"
   },
   "source": [
    "### 5.2 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xKhMqThC3RfR"
   },
   "outputs": [],
   "source": [
    "class RNABindingLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for RNA sequence binding prediction.\n",
    "\n",
    "    Architecture:\n",
    "    - 2-layer bidirectional LSTM\n",
    "    - Batch normalization\n",
    "    - Dropout for regularization\n",
    "    - 2 fully connected layers for regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=4, hidden_dim=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super(RNABindingLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim * self.num_directions)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim * self.num_directions, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch, seq_length, 4)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Concatenate last hidden states from forward and backward\n",
    "            hidden_forward = hidden[-2, :, :]\n",
    "            hidden_backward = hidden[-1, :, :]\n",
    "            combined = torch.cat([hidden_forward, hidden_backward], dim=1)\n",
    "        else:\n",
    "            combined = hidden[-1, :, :]\n",
    "\n",
    "        combined = self.bn(combined)\n",
    "        combined = self.dropout(combined)\n",
    "\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxGTwukW3RfR"
   },
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F3Qx6YCy3RfR"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch and return loss and correlation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_masks = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, y, mask = batch\n",
    "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = masked_mse_loss(predictions, y, mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Collect for metric calculation\n",
    "        all_preds.append(predictions.detach().cpu())\n",
    "        all_targets.append(y.detach().cpu())\n",
    "        all_masks.append(mask.detach().cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_masks = torch.cat(all_masks, dim=0)\n",
    "    \n",
    "    spearman_corr = masked_spearman_correlation(all_preds, all_targets, all_masks)\n",
    "\n",
    "    return total_loss / num_batches, spearman_corr.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluate model and return loss and Spearman correlation.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, y, mask = batch\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "            predictions = model(x)\n",
    "            loss = masked_mse_loss(predictions, y, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_targets.append(y.cpu())\n",
    "            all_masks.append(mask.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_masks = torch.cat(all_masks, dim=0)\n",
    "\n",
    "    spearman_corr = masked_spearman_correlation(all_preds, all_targets, all_masks)\n",
    "\n",
    "    return total_loss / num_batches, spearman_corr.item()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device,\n",
    "                num_epochs, model_name, patience=15, save_every=10):\n",
    "    \"\"\"Full training loop with early stopping and periodic checkpoints.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    train_losses = []\n",
    "    train_correlations = []\n",
    "    val_losses = []\n",
    "    val_correlations = []\n",
    "\n",
    "    best_val_corr = -float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        train_loss, train_corr = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_corr = evaluate(model, val_loader, device)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_corr)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_correlations.append(train_corr)\n",
    "        val_losses.append(val_loss)\n",
    "        val_correlations.append(val_corr)\n",
    "\n",
    "        if val_corr > best_val_corr:\n",
    "            best_val_corr = val_corr\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "            # Save best model to Output directory\n",
    "            torch.save(best_model_state, f'{OUTPUT_DIR}/{model_name}_best_t3.pth')\n",
    "            print(f\"  → Saved new best model (Spearman: {val_corr:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train Spearman: {train_corr:.4f} | \"\n",
    "                  f\"Val Spearman: {val_corr:.4f} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # Periodic checkpoint every N epochs\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'train_correlations': train_correlations,\n",
    "                'val_losses': val_losses,\n",
    "                'val_correlations': val_correlations,\n",
    "                'best_val_corr': best_val_corr\n",
    "            }\n",
    "            torch.save(checkpoint, f'{OUTPUT_DIR}/{model_name}_checkpoint_epoch{epoch+1}_t3.pth')\n",
    "            print(f\"  → Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
    "    print(f\"Best validation Spearman correlation: {best_val_corr:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_correlations': train_correlations,\n",
    "        'val_losses': val_losses,\n",
    "        'val_correlations': val_correlations,\n",
    "        'best_val_corr': best_val_corr,\n",
    "        'training_time': total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvPBHoqu3RfR"
   },
   "source": [
    "## 7. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LG_WFqWc3RfR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for protein: RBFOX1\n",
      "Found cached data for RBFOX1 (train). Loading from data/RBFOX1_train_data.pt...\n",
      "Found cached data for RBFOX1 (val). Loading from data/RBFOX1_val_data.pt...\n",
      "Found cached data for RBFOX1 (test). Loading from data/RBFOX1_test_data.pt...\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 96261\n",
      "  Val: 24065\n",
      "  Test: 121031\n"
     ]
    }
   ],
   "source": [
    "PROTEIN_NAME = 'RBFOX1'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading data for protein: {PROTEIN_NAME}\")\n",
    "config = RNAConfig()\n",
    "\n",
    "train_dataset = load_rnacompete_data(PROTEIN_NAME, split='train', config=config)\n",
    "val_dataset = load_rnacompete_data(PROTEIN_NAME, split='val', config=config)\n",
    "test_dataset = load_rnacompete_data(PROTEIN_NAME, split='test', config=config)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XflxQ1VL3RfR"
   },
   "source": [
    "## 8. Hyperparameter Search and Training\n",
    "\n",
    "Here we define the search space and a function to iterate through hyperparameters, training the model for each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROBUST GRID SEARCH (12 Combinations) ---\n",
    "# for train_model with patience 7, and scheduler patience 5\n",
    "# POSSIBLE HYPERPARAMETER TO ADD- SHCEDULER PATIENCE, INFLUENCES HOW LR CHANGES\n",
    "cnn_param_grid = {\n",
    "    'learning_rate': [0.007, 0.009],\n",
    "    'hidden_dim': [512],   # 256 was better than 128, so we test 512\n",
    "    'dropout': [0.2, 0.3]       # Lower dropout preferred for CNNs\n",
    "}\n",
    "\n",
    "lstm_param_grid = {\n",
    "    'learning_rate': [0.0001, 0.002],\n",
    "    'hidden_dim': [256],   # Larger LSTMs often capture complex motifs better\n",
    "    'dropout': [0.3, 0.4]       # Standard regularization for LSTMs\n",
    "}\n",
    "\n",
    "#simply fast grid to test if code runs well\n",
    "# param_grid = {\n",
    "#     # Covers best LSTM (0.001), best CNN (0.0001) and midpoint (0.0005)\n",
    "#     'learning_rate': [0.0001],\n",
    "#     # Tests if doubling capacity improves results\n",
    "#     'hidden_dim': [2],\n",
    "#     # Standard regularization vs Stronger regularization (for larger models)\n",
    "#     'dropout': [0.8]\n",
    "# }\n",
    "\n",
    "def hyperparameter_search(model_class, model_name, param_grid, train_loader, val_loader, device, epochs=50):\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_val_corr = -float('inf')\n",
    "    best_config = None\n",
    "    best_history = None\n",
    "    \n",
    "    print(f\"Starting Hyperparameter Search for {model_name} with {len(experiments)} configurations...\")\n",
    "    \n",
    "    results = []\n",
    "    # Define a path for the intermediate results\n",
    "    results_path = f'{OUTPUT_DIR}/{model_name}_search_results_t3.json'\n",
    "    \n",
    "    for i, config in enumerate(experiments):\n",
    "        print(f\"\\nRunning experiment {i+1}/{len(experiments)}: {config}\")\n",
    "        \n",
    "        configure_seed(42)\n",
    "        \n",
    "        # Initialize model with current config\n",
    "        # Note: LSTM and CNN have slightly different init signatures if not careful,\n",
    "        # but here we matched the param names in the grid to the init args\n",
    "        if model_name == 'LSTM':\n",
    "             model = model_class(\n",
    "                hidden_dim=config['hidden_dim'],\n",
    "                dropout=config['dropout']\n",
    "            ).to(device)\n",
    "        else:\n",
    "             model = model_class(\n",
    "                hidden_dim=config['hidden_dim'],\n",
    "                dropout=config['dropout']\n",
    "            ).to(device)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        # Train - using fewer epochs for search to save time (can increase for final result)\n",
    "        # We use a unique name for temporary checkpoints\n",
    "        temp_name = f\"{model_name}_exp{i}\"\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader, optimizer, scheduler,\n",
    "            device, num_epochs=epochs, model_name=temp_name, patience=7, save_every=100\n",
    "        )\n",
    "        \n",
    "        # --- SAVE PROGRESS IMMEDIATELY ---\n",
    "        run_result = {\n",
    "            'experiment_index': i,\n",
    "            'config': config,\n",
    "            'best_val_corr': history['best_val_corr'],\n",
    "            'training_time': history['training_time'],\n",
    "            'history': history # Full history included for plotting/resuming\n",
    "        }\n",
    "        results.append(run_result)\n",
    "        \n",
    "        try:\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            print(f\"  → Search progress saved to {results_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  → Warning: Could not save progress to JSON: {e}\")\n",
    "\n",
    "        if history['best_val_corr'] > best_val_corr:\n",
    "            best_val_corr = history['best_val_corr']\n",
    "            best_config = config\n",
    "            best_history = history\n",
    "            \n",
    "            # Save the best model of the search as the final best model\n",
    "            # We load the best state from the temp file and save it to the final destination\n",
    "            best_state = torch.load(f'{OUTPUT_DIR}/{temp_name}_best_t3.pth')\n",
    "            torch.save(best_state, f'{OUTPUT_DIR}/best_{model_name.lower()}_model_t3.pth')\n",
    "            \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Best {model_name} configuration: {best_config}\")\n",
    "    print(f\"Best validation Spearman: {best_val_corr:.4f}\")\n",
    "    print(f\"Best model saved to {OUTPUT_DIR}/best_{model_name.lower()}_model_t3.pth\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return best_config, best_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning CNN ----\n",
      "Starting Hyperparameter Search for CNN with 4 configurations...\n",
      "\n",
      "Running experiment 1/4: {'learning_rate': 0.007, 'hidden_dim': 512, 'dropout': 0.2}\n",
      "\n",
      "============================================================\n",
      "Training CNN_exp0\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.5349)\n",
      "Epoch   1/50 | Train Loss: 1.8975 | Val Loss: 0.5042 | Train Spearman: 0.4029 | Val Spearman: 0.5349 | Time: 17.37s\n",
      "  → Saved new best model (Spearman: 0.5611)\n",
      "  → Saved new best model (Spearman: 0.5873)\n",
      "  → Saved new best model (Spearman: 0.6000)\n",
      "  → Saved new best model (Spearman: 0.6077)\n",
      "Epoch   5/50 | Train Loss: 0.4693 | Val Loss: 0.4486 | Train Spearman: 0.5482 | Val Spearman: 0.6077 | Time: 9.67s\n",
      "  → Saved new best model (Spearman: 0.6086)\n",
      "  → Saved new best model (Spearman: 0.6154)\n",
      "  → Saved new best model (Spearman: 0.6181)\n",
      "Epoch  10/50 | Train Loss: 0.4374 | Val Loss: 0.3903 | Train Spearman: 0.5702 | Val Spearman: 0.6181 | Time: 9.69s\n",
      "  → Saved new best model (Spearman: 0.6286)\n",
      "Epoch  15/50 | Train Loss: 0.4292 | Val Loss: 0.4250 | Train Spearman: 0.5782 | Val Spearman: 0.6082 | Time: 9.17s\n",
      "  → Saved new best model (Spearman: 0.6352)\n",
      "  → Saved new best model (Spearman: 0.6423)\n",
      "Epoch  20/50 | Train Loss: 0.3912 | Val Loss: 0.4037 | Train Spearman: 0.6012 | Val Spearman: 0.6423 | Time: 11.90s\n",
      "  → Saved new best model (Spearman: 0.6428)\n",
      "Epoch  25/50 | Train Loss: 0.3778 | Val Loss: 0.4000 | Train Spearman: 0.6095 | Val Spearman: 0.6401 | Time: 12.29s\n",
      "  → Saved new best model (Spearman: 0.6443)\n",
      "  → Saved new best model (Spearman: 0.6471)\n",
      "  → Saved new best model (Spearman: 0.6476)\n",
      "Epoch  30/50 | Train Loss: 0.3752 | Val Loss: 0.3884 | Train Spearman: 0.6100 | Val Spearman: 0.6402 | Time: 12.39s\n",
      "  → Saved new best model (Spearman: 0.6516)\n",
      "  → Saved new best model (Spearman: 0.6522)\n",
      "Epoch  35/50 | Train Loss: 0.3702 | Val Loss: 0.4219 | Train Spearman: 0.6151 | Val Spearman: 0.6473 | Time: 15.83s\n",
      "  → Saved new best model (Spearman: 0.6537)\n",
      "Epoch  40/50 | Train Loss: 0.3666 | Val Loss: 0.3648 | Train Spearman: 0.6143 | Val Spearman: 0.6436 | Time: 11.53s\n",
      "Epoch  45/50 | Train Loss: 0.3481 | Val Loss: 0.3566 | Train Spearman: 0.6245 | Val Spearman: 0.6499 | Time: 9.59s\n",
      "\n",
      "Early stopping at epoch 45\n",
      "\n",
      "Training completed in 516.74s\n",
      "Best validation Spearman correlation: 0.6537\n",
      "  → Search progress saved to Outputs_t3/CNN_search_results_t3.json\n",
      "\n",
      "Running experiment 2/4: {'learning_rate': 0.007, 'hidden_dim': 512, 'dropout': 0.3}\n",
      "\n",
      "============================================================\n",
      "Training CNN_exp1\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.5301)\n",
      "Epoch   1/50 | Train Loss: 1.8852 | Val Loss: 0.5048 | Train Spearman: 0.4061 | Val Spearman: 0.5301 | Time: 12.01s\n",
      "  → Saved new best model (Spearman: 0.5515)\n",
      "  → Saved new best model (Spearman: 0.5750)\n",
      "  → Saved new best model (Spearman: 0.5764)\n",
      "  → Saved new best model (Spearman: 0.5836)\n",
      "Epoch   5/50 | Train Loss: 0.5173 | Val Loss: 0.5161 | Train Spearman: 0.5187 | Val Spearman: 0.5836 | Time: 9.79s\n",
      "  → Saved new best model (Spearman: 0.6101)\n",
      "Epoch  10/50 | Train Loss: 0.4678 | Val Loss: 0.4153 | Train Spearman: 0.5509 | Val Spearman: 0.6060 | Time: 9.48s\n",
      "  → Saved new best model (Spearman: 0.6197)\n",
      "Epoch  15/50 | Train Loss: 0.4641 | Val Loss: 0.4245 | Train Spearman: 0.5530 | Val Spearman: 0.6145 | Time: 12.43s\n",
      "  → Saved new best model (Spearman: 0.6259)\n",
      "Epoch  20/50 | Train Loss: 0.4259 | Val Loss: 0.5225 | Train Spearman: 0.5807 | Val Spearman: 0.6140 | Time: 12.15s\n",
      "  → Saved new best model (Spearman: 0.6302)\n",
      "Epoch  25/50 | Train Loss: 0.4143 | Val Loss: 0.4829 | Train Spearman: 0.5884 | Val Spearman: 0.6233 | Time: 18.28s\n",
      "  → Saved new best model (Spearman: 0.6315)\n",
      "  → Saved new best model (Spearman: 0.6324)\n",
      "  → Saved new best model (Spearman: 0.6339)\n",
      "Epoch  30/50 | Train Loss: 0.4106 | Val Loss: 0.4571 | Train Spearman: 0.5882 | Val Spearman: 0.6306 | Time: 9.60s\n",
      "  → Saved new best model (Spearman: 0.6340)\n",
      "  → Saved new best model (Spearman: 0.6354)\n",
      "  → Saved new best model (Spearman: 0.6397)\n",
      "Epoch  35/50 | Train Loss: 0.4058 | Val Loss: 0.4524 | Train Spearman: 0.5876 | Val Spearman: 0.6311 | Time: 9.84s\n",
      "Epoch  40/50 | Train Loss: 0.4068 | Val Loss: 0.3910 | Train Spearman: 0.5912 | Val Spearman: 0.6362 | Time: 9.12s\n",
      "  → Saved new best model (Spearman: 0.6450)\n",
      "  → Saved new best model (Spearman: 0.6460)\n",
      "  → Saved new best model (Spearman: 0.6467)\n",
      "Epoch  45/50 | Train Loss: 0.3743 | Val Loss: 0.3997 | Train Spearman: 0.6104 | Val Spearman: 0.6467 | Time: 12.13s\n",
      "  → Saved new best model (Spearman: 0.6471)\n",
      "  → Saved new best model (Spearman: 0.6483)\n",
      "Epoch  50/50 | Train Loss: 0.3717 | Val Loss: 0.4218 | Train Spearman: 0.6114 | Val Spearman: 0.6480 | Time: 12.35s\n",
      "\n",
      "Training completed in 608.94s\n",
      "Best validation Spearman correlation: 0.6483\n",
      "  → Search progress saved to Outputs_t3/CNN_search_results_t3.json\n",
      "\n",
      "Running experiment 3/4: {'learning_rate': 0.009, 'hidden_dim': 512, 'dropout': 0.2}\n",
      "\n",
      "============================================================\n",
      "Training CNN_exp2\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.5123)\n",
      "Epoch   1/50 | Train Loss: 2.3554 | Val Loss: 0.6938 | Train Spearman: 0.4257 | Val Spearman: 0.5123 | Time: 8.81s\n",
      "  → Saved new best model (Spearman: 0.5554)\n",
      "  → Saved new best model (Spearman: 0.5764)\n",
      "  → Saved new best model (Spearman: 0.5864)\n",
      "Epoch   5/50 | Train Loss: 0.4830 | Val Loss: 0.4381 | Train Spearman: 0.5343 | Val Spearman: 0.5864 | Time: 11.82s\n",
      "  → Saved new best model (Spearman: 0.5968)\n",
      "  → Saved new best model (Spearman: 0.6024)\n",
      "  → Saved new best model (Spearman: 0.6078)\n",
      "  → Saved new best model (Spearman: 0.6139)\n",
      "Epoch  10/50 | Train Loss: 0.4584 | Val Loss: 0.3938 | Train Spearman: 0.5519 | Val Spearman: 0.6139 | Time: 12.13s\n",
      "Epoch  15/50 | Train Loss: 0.4513 | Val Loss: 0.3959 | Train Spearman: 0.5594 | Val Spearman: 0.6082 | Time: 9.36s\n",
      "  → Saved new best model (Spearman: 0.6295)\n",
      "  → Saved new best model (Spearman: 0.6324)\n",
      "  → Saved new best model (Spearman: 0.6335)\n",
      "Epoch  20/50 | Train Loss: 0.4046 | Val Loss: 0.4278 | Train Spearman: 0.5906 | Val Spearman: 0.6335 | Time: 9.70s\n",
      "  → Saved new best model (Spearman: 0.6338)\n",
      "  → Saved new best model (Spearman: 0.6349)\n",
      "Epoch  25/50 | Train Loss: 0.3960 | Val Loss: 0.4829 | Train Spearman: 0.5959 | Val Spearman: 0.6273 | Time: 9.20s\n",
      "  → Saved new best model (Spearman: 0.6398)\n",
      "Epoch  30/50 | Train Loss: 0.3924 | Val Loss: 0.3723 | Train Spearman: 0.5970 | Val Spearman: 0.6324 | Time: 11.79s\n",
      "  → Saved new best model (Spearman: 0.6446)\n",
      "  → Saved new best model (Spearman: 0.6479)\n",
      "Epoch  35/50 | Train Loss: 0.3730 | Val Loss: 0.4056 | Train Spearman: 0.6103 | Val Spearman: 0.6479 | Time: 12.54s\n",
      "  → Saved new best model (Spearman: 0.6482)\n",
      "Epoch  40/50 | Train Loss: 0.3608 | Val Loss: 0.3557 | Train Spearman: 0.6129 | Val Spearman: 0.6465 | Time: 11.81s\n",
      "  → Saved new best model (Spearman: 0.6490)\n",
      "  → Saved new best model (Spearman: 0.6494)\n",
      "Epoch  45/50 | Train Loss: 0.3550 | Val Loss: 0.3908 | Train Spearman: 0.6183 | Val Spearman: 0.6375 | Time: 11.96s\n",
      "  → Saved new best model (Spearman: 0.6552)\n",
      "Epoch  50/50 | Train Loss: 0.3404 | Val Loss: 0.3654 | Train Spearman: 0.6264 | Val Spearman: 0.6552 | Time: 10.03s\n",
      "\n",
      "Training completed in 580.41s\n",
      "Best validation Spearman correlation: 0.6552\n",
      "  → Search progress saved to Outputs_t3/CNN_search_results_t3.json\n",
      "\n",
      "Running experiment 4/4: {'learning_rate': 0.009, 'hidden_dim': 512, 'dropout': 0.3}\n",
      "\n",
      "============================================================\n",
      "Training CNN_exp3\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.5315)\n",
      "Epoch   1/50 | Train Loss: 3.1502 | Val Loss: 0.5226 | Train Spearman: 0.4148 | Val Spearman: 0.5315 | Time: 12.20s\n",
      "  → Saved new best model (Spearman: 0.5564)\n",
      "  → Saved new best model (Spearman: 0.5606)\n",
      "  → Saved new best model (Spearman: 0.5666)\n",
      "  → Saved new best model (Spearman: 0.5756)\n",
      "Epoch   5/50 | Train Loss: 0.5127 | Val Loss: 0.4609 | Train Spearman: 0.5102 | Val Spearman: 0.5756 | Time: 9.53s\n",
      "  → Saved new best model (Spearman: 0.5838)\n",
      "  → Saved new best model (Spearman: 0.5903)\n",
      "  → Saved new best model (Spearman: 0.5928)\n",
      "Epoch  10/50 | Train Loss: 0.4956 | Val Loss: 0.4198 | Train Spearman: 0.5263 | Val Spearman: 0.5928 | Time: 10.50s\n",
      "  → Saved new best model (Spearman: 0.5937)\n",
      "  → Saved new best model (Spearman: 0.5952)\n",
      "  → Saved new best model (Spearman: 0.6016)\n",
      "Epoch  15/50 | Train Loss: 0.4885 | Val Loss: 0.4489 | Train Spearman: 0.5309 | Val Spearman: 0.6016 | Time: 12.03s\n",
      "Epoch  20/50 | Train Loss: 0.4847 | Val Loss: 0.5777 | Train Spearman: 0.5354 | Val Spearman: 0.5873 | Time: 12.34s\n",
      "  → Saved new best model (Spearman: 0.6026)\n",
      "Epoch  25/50 | Train Loss: 0.4819 | Val Loss: 0.4922 | Train Spearman: 0.5378 | Val Spearman: 0.5897 | Time: 12.12s\n",
      "  → Saved new best model (Spearman: 0.6078)\n",
      "  → Saved new best model (Spearman: 0.6137)\n",
      "Epoch  30/50 | Train Loss: 0.4462 | Val Loss: 0.4194 | Train Spearman: 0.5622 | Val Spearman: 0.6121 | Time: 12.40s\n",
      "  → Saved new best model (Spearman: 0.6163)\n",
      "  → Saved new best model (Spearman: 0.6178)\n",
      "Epoch  35/50 | Train Loss: 0.4355 | Val Loss: 0.5224 | Train Spearman: 0.5646 | Val Spearman: 0.6178 | Time: 12.05s\n",
      "  → Saved new best model (Spearman: 0.6193)\n",
      "  → Saved new best model (Spearman: 0.6195)\n",
      "  → Saved new best model (Spearman: 0.6234)\n",
      "Epoch  40/50 | Train Loss: 0.4293 | Val Loss: 0.4068 | Train Spearman: 0.5702 | Val Spearman: 0.6224 | Time: 9.45s\n",
      "Epoch  45/50 | Train Loss: 0.4254 | Val Loss: 0.4498 | Train Spearman: 0.5770 | Val Spearman: 0.6156 | Time: 14.45s\n",
      "  → Saved new best model (Spearman: 0.6341)\n",
      "Epoch  50/50 | Train Loss: 0.3939 | Val Loss: 0.4319 | Train Spearman: 0.5913 | Val Spearman: 0.6303 | Time: 13.29s\n",
      "\n",
      "Training completed in 579.95s\n",
      "Best validation Spearman correlation: 0.6341\n",
      "  → Search progress saved to Outputs_t3/CNN_search_results_t3.json\n",
      "\n",
      "============================================================\n",
      "Best CNN configuration: {'learning_rate': 0.009, 'hidden_dim': 512, 'dropout': 0.2}\n",
      "Best validation Spearman: 0.6552\n",
      "Best model saved to Outputs_t3/best_cnn_model_t3.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Train and Search CNN\n",
    "print(\"---- Tuning CNN ----\")\n",
    "best_cnn_config, cnn_history = hyperparameter_search(\n",
    "    RNABindingCNN, 'CNN', cnn_param_grid, train_loader, val_loader, device, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning LSTM ----\n",
      "Starting Hyperparameter Search for LSTM with 4 configurations...\n",
      "\n",
      "Running experiment 1/4: {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.3}\n",
      "\n",
      "============================================================\n",
      "Training LSTM_exp0\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.2933)\n",
      "Epoch   1/50 | Train Loss: 0.9862 | Val Loss: 0.9510 | Train Spearman: 0.1902 | Val Spearman: 0.2933 | Time: 29.44s\n",
      "  → Saved new best model (Spearman: 0.3674)\n",
      "  → Saved new best model (Spearman: 0.4078)\n",
      "  → Saved new best model (Spearman: 0.4134)\n",
      "  → Saved new best model (Spearman: 0.4325)\n",
      "Epoch   5/50 | Train Loss: 0.7992 | Val Loss: 0.9510 | Train Spearman: 0.4231 | Val Spearman: 0.4325 | Time: 26.96s\n",
      "  → Saved new best model (Spearman: 0.4740)\n",
      "  → Saved new best model (Spearman: 0.4995)\n",
      "  → Saved new best model (Spearman: 0.5144)\n",
      "  → Saved new best model (Spearman: 0.5248)\n",
      "  → Saved new best model (Spearman: 0.5532)\n",
      "Epoch  10/50 | Train Loss: 0.5081 | Val Loss: 0.5113 | Train Spearman: 0.5336 | Val Spearman: 0.5532 | Time: 34.31s\n",
      "  → Saved new best model (Spearman: 0.5562)\n",
      "  → Saved new best model (Spearman: 0.5906)\n",
      "  → Saved new best model (Spearman: 0.5971)\n",
      "Epoch  15/50 | Train Loss: 0.4379 | Val Loss: 0.4337 | Train Spearman: 0.5734 | Val Spearman: 0.5971 | Time: 27.30s\n",
      "  → Saved new best model (Spearman: 0.6079)\n",
      "  → Saved new best model (Spearman: 0.6119)\n",
      "  → Saved new best model (Spearman: 0.6155)\n",
      "Epoch  20/50 | Train Loss: 0.4097 | Val Loss: 0.3942 | Train Spearman: 0.5935 | Val Spearman: 0.6155 | Time: 26.49s\n",
      "  → Saved new best model (Spearman: 0.6266)\n",
      "Epoch  25/50 | Train Loss: 0.3951 | Val Loss: 0.4250 | Train Spearman: 0.6049 | Val Spearman: 0.5789 | Time: 24.32s\n",
      "  → Saved new best model (Spearman: 0.6290)\n",
      "Epoch  30/50 | Train Loss: 0.3720 | Val Loss: 0.3820 | Train Spearman: 0.6166 | Val Spearman: 0.6290 | Time: 25.31s\n",
      "  → Saved new best model (Spearman: 0.6364)\n",
      "  → Saved new best model (Spearman: 0.6406)\n",
      "Epoch  35/50 | Train Loss: 0.3643 | Val Loss: 0.3811 | Train Spearman: 0.6256 | Val Spearman: 0.6391 | Time: 23.99s\n",
      "  → Saved new best model (Spearman: 0.6467)\n",
      "Epoch  40/50 | Train Loss: 0.3503 | Val Loss: 0.3580 | Train Spearman: 0.6325 | Val Spearman: 0.6367 | Time: 28.49s\n",
      "  → Saved new best model (Spearman: 0.6545)\n",
      "Epoch  45/50 | Train Loss: 0.3314 | Val Loss: 0.3457 | Train Spearman: 0.6440 | Val Spearman: 0.6519 | Time: 25.55s\n",
      "  → Saved new best model (Spearman: 0.6546)\n",
      "  → Saved new best model (Spearman: 0.6556)\n",
      "  → Saved new best model (Spearman: 0.6565)\n",
      "Epoch  50/50 | Train Loss: 0.3253 | Val Loss: 0.3428 | Train Spearman: 0.6474 | Val Spearman: 0.6565 | Time: 24.81s\n",
      "\n",
      "Training completed in 1342.18s\n",
      "Best validation Spearman correlation: 0.6565\n",
      "  → Search progress saved to Outputs_t3/LSTM_search_results_t3.json\n",
      "\n",
      "Running experiment 2/4: {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.4}\n",
      "\n",
      "============================================================\n",
      "Training LSTM_exp1\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.2893)\n",
      "Epoch   1/50 | Train Loss: 1.0019 | Val Loss: 0.9478 | Train Spearman: 0.1671 | Val Spearman: 0.2893 | Time: 25.01s\n",
      "  → Saved new best model (Spearman: 0.3486)\n",
      "  → Saved new best model (Spearman: 0.4010)\n",
      "  → Saved new best model (Spearman: 0.4146)\n",
      "  → Saved new best model (Spearman: 0.4354)\n",
      "Epoch   5/50 | Train Loss: 0.8043 | Val Loss: 0.8170 | Train Spearman: 0.4187 | Val Spearman: 0.4354 | Time: 25.47s\n",
      "  → Saved new best model (Spearman: 0.4632)\n",
      "  → Saved new best model (Spearman: 0.4869)\n",
      "  → Saved new best model (Spearman: 0.4990)\n",
      "  → Saved new best model (Spearman: 0.5192)\n",
      "  → Saved new best model (Spearman: 0.5454)\n",
      "Epoch  10/50 | Train Loss: 0.5227 | Val Loss: 0.5015 | Train Spearman: 0.5185 | Val Spearman: 0.5454 | Time: 24.08s\n",
      "  → Saved new best model (Spearman: 0.5526)\n",
      "  → Saved new best model (Spearman: 0.5599)\n",
      "  → Saved new best model (Spearman: 0.5679)\n",
      "  → Saved new best model (Spearman: 0.5727)\n",
      "Epoch  15/50 | Train Loss: 0.4549 | Val Loss: 0.4838 | Train Spearman: 0.5638 | Val Spearman: 0.5727 | Time: 25.08s\n",
      "  → Saved new best model (Spearman: 0.5734)\n",
      "  → Saved new best model (Spearman: 0.5965)\n",
      "  → Saved new best model (Spearman: 0.6019)\n",
      "  → Saved new best model (Spearman: 0.6122)\n",
      "Epoch  20/50 | Train Loss: 0.4189 | Val Loss: 0.3988 | Train Spearman: 0.5876 | Val Spearman: 0.6122 | Time: 24.50s\n",
      "  → Saved new best model (Spearman: 0.6224)\n",
      "Epoch  25/50 | Train Loss: 0.4007 | Val Loss: 0.4073 | Train Spearman: 0.6021 | Val Spearman: 0.6205 | Time: 25.59s\n",
      "  → Saved new best model (Spearman: 0.6239)\n",
      "  → Saved new best model (Spearman: 0.6276)\n",
      "Epoch  30/50 | Train Loss: 0.3760 | Val Loss: 0.3940 | Train Spearman: 0.6144 | Val Spearman: 0.6265 | Time: 24.18s\n",
      "  → Saved new best model (Spearman: 0.6356)\n",
      "Epoch  35/50 | Train Loss: 0.3711 | Val Loss: 0.3983 | Train Spearman: 0.6212 | Val Spearman: 0.6260 | Time: 25.64s\n",
      "  → Saved new best model (Spearman: 0.6377)\n",
      "  → Saved new best model (Spearman: 0.6382)\n",
      "Epoch  40/50 | Train Loss: 0.3570 | Val Loss: 0.3619 | Train Spearman: 0.6284 | Val Spearman: 0.6358 | Time: 24.71s\n",
      "  → Saved new best model (Spearman: 0.6457)\n",
      "  → Saved new best model (Spearman: 0.6481)\n",
      "Epoch  45/50 | Train Loss: 0.3495 | Val Loss: 0.3582 | Train Spearman: 0.6334 | Val Spearman: 0.6386 | Time: 24.70s\n",
      "  → Saved new best model (Spearman: 0.6506)\n",
      "Epoch  50/50 | Train Loss: 0.3434 | Val Loss: 0.3532 | Train Spearman: 0.6383 | Val Spearman: 0.6476 | Time: 25.26s\n",
      "\n",
      "Training completed in 1263.31s\n",
      "Best validation Spearman correlation: 0.6506\n",
      "  → Search progress saved to Outputs_t3/LSTM_search_results_t3.json\n",
      "\n",
      "Running experiment 3/4: {'learning_rate': 0.002, 'hidden_dim': 256, 'dropout': 0.3}\n",
      "\n",
      "============================================================\n",
      "Training LSTM_exp2\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.0568)\n",
      "Epoch   1/50 | Train Loss: 1.0061 | Val Loss: 1.0025 | Train Spearman: 0.0478 | Val Spearman: 0.0568 | Time: 25.69s\n",
      "Epoch   5/50 | Train Loss: 1.0001 | Val Loss: 1.0006 | Train Spearman: 0.0020 | Val Spearman: 0.0036 | Time: 24.65s\n",
      "  → Saved new best model (Spearman: 0.1203)\n",
      "Epoch  10/50 | Train Loss: 1.0003 | Val Loss: 1.0018 | Train Spearman: 0.0030 | Val Spearman: 0.0117 | Time: 25.21s\n",
      "  → Saved new best model (Spearman: 0.1735)\n",
      "Epoch  15/50 | Train Loss: 1.0002 | Val Loss: 1.0008 | Train Spearman: 0.0018 | Val Spearman: -0.0112 | Time: 24.84s\n",
      "Epoch  20/50 | Train Loss: 0.9995 | Val Loss: 1.0009 | Train Spearman: 0.0120 | Val Spearman: -0.1261 | Time: 37.74s\n",
      "\n",
      "Early stopping at epoch 20\n",
      "\n",
      "Training completed in 560.28s\n",
      "Best validation Spearman correlation: 0.1735\n",
      "  → Search progress saved to Outputs_t3/LSTM_search_results_t3.json\n",
      "\n",
      "Running experiment 4/4: {'learning_rate': 0.002, 'hidden_dim': 256, 'dropout': 0.4}\n",
      "\n",
      "============================================================\n",
      "Training LSTM_exp3\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.0919)\n",
      "Epoch   1/50 | Train Loss: 1.0178 | Val Loss: 1.0016 | Train Spearman: 0.0135 | Val Spearman: 0.0919 | Time: 29.92s\n",
      "Epoch   5/50 | Train Loss: 1.0001 | Val Loss: 1.0006 | Train Spearman: 0.0020 | Val Spearman: 0.0036 | Time: 44.92s\n",
      "\n",
      "Early stopping at epoch 8\n",
      "\n",
      "Training completed in 331.61s\n",
      "Best validation Spearman correlation: 0.0919\n",
      "  → Search progress saved to Outputs_t3/LSTM_search_results_t3.json\n",
      "\n",
      "============================================================\n",
      "Best LSTM configuration: {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.3}\n",
      "Best validation Spearman: 0.6565\n",
      "Best model saved to Outputs_t3/best_lstm_model_t3.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8.2 Train and Search LSTM\n",
    "print(\"---- Tuning LSTM ----\")\n",
    "best_lstm_config, lstm_history = hyperparameter_search(\n",
    "    RNABindingLSTM, 'LSTM', lstm_param_grid, train_loader, val_loader, device, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlEimsfj3RfR"
   },
   "source": [
    "## 9. Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ax4WTYYe3RfR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best CNN Test Results:\n",
      "  Test Loss: 0.3633\n",
      "  Test Spearman Correlation: 0.6601\n"
     ]
    }
   ],
   "source": [
    "# Load best CNN model\n",
    "cnn_model = RNABindingCNN(\n",
    "    hidden_dim=best_cnn_config['hidden_dim'], \n",
    "    dropout=best_cnn_config['dropout']\n",
    ").to(device)\n",
    "cnn_model.load_state_dict(torch.load(f'{OUTPUT_DIR}/best_cnn_model_t3.pth'))\n",
    "\n",
    "# Evaluate CNN on test set\n",
    "cnn_test_loss, cnn_test_corr = evaluate(cnn_model, test_loader, device)\n",
    "print(f\"\\nBest CNN Test Results:\")\n",
    "print(f\"  Test Loss: {cnn_test_loss:.4f}\")\n",
    "print(f\"  Test Spearman Correlation: {cnn_test_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VKvfoFQ53RfR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LSTM Test Results:\n",
      "  Test Loss: 0.3423\n",
      "  Test Spearman Correlation: 0.6576\n"
     ]
    }
   ],
   "source": [
    "# Load best LSTM model\n",
    "lstm_model = RNABindingLSTM(\n",
    "    hidden_dim=best_lstm_config['hidden_dim'], \n",
    "    dropout=best_lstm_config['dropout']\n",
    ").to(device)\n",
    "lstm_model.load_state_dict(torch.load(f'{OUTPUT_DIR}/best_lstm_model_t3.pth'))\n",
    "\n",
    "# Evaluate LSTM on test set\n",
    "lstm_test_loss, lstm_test_corr = evaluate(lstm_model, test_loader, device)\n",
    "print(f\"\\nBest LSTM Test Results:\")\n",
    "print(f\"  Test Loss: {lstm_test_loss:.4f}\")\n",
    "print(f\"  Test Spearman Correlation: {lstm_test_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0hVYj_I3RfR"
   },
   "source": [
    "## 10. Plotting Results and Saving History\n",
    "\n",
    "Saving plots and full experiment history to JSON for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pxyLw6-33RfR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete experiment data saved to 'Outputs_t3/complete_experiment_data_t3.json'\n",
      "Plots saved to 'loss_plot.png' and 'accuracy_plot.png'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_and_plot_results(cnn_hist, lstm_hist, cnn_config, lstm_config, cnn_test_res, lstm_test_res):\n",
    "    # Unpack test results\n",
    "    cnn_test_loss, cnn_test_corr = cnn_test_res\n",
    "    lstm_test_loss, lstm_test_corr = lstm_test_res\n",
    "    \n",
    "    # 1. Prepare Data for JSON Storage\n",
    "    experiment_data = {\n",
    "        \"CNN\": {\n",
    "            \"parameters\": cnn_config,\n",
    "            \"training_time_seconds\": cnn_hist['training_time'],\n",
    "            \"best_val_accuracy\": cnn_hist['best_val_corr'],\n",
    "            \"best_val_epoch\": int(np.argmax(cnn_hist['val_correlations']) + 1),\n",
    "            \"test_loss\": cnn_test_loss,\n",
    "            \"test_accuracy\": cnn_test_corr,\n",
    "            \"history\": {\n",
    "                \"train_losses\": cnn_hist['train_losses'],\n",
    "                \"train_correlations\": cnn_hist['train_correlations'],\n",
    "                \"val_losses\": cnn_hist['val_losses'],\n",
    "                \"val_correlations\": cnn_hist['val_correlations']\n",
    "            }\n",
    "        },\n",
    "        \"LSTM\": {\n",
    "            \"parameters\": lstm_config,\n",
    "            \"training_time_seconds\": lstm_hist['training_time'],\n",
    "            \"best_val_accuracy\": lstm_hist['best_val_corr'],\n",
    "            \"best_val_epoch\": int(np.argmax(lstm_hist['val_correlations']) + 1),\n",
    "            \"test_loss\": lstm_test_loss,\n",
    "            \"test_accuracy\": lstm_test_corr,\n",
    "            \"history\": {\n",
    "                \"train_losses\": lstm_hist['train_losses'],\n",
    "                \"train_correlations\": lstm_hist['train_correlations'],\n",
    "                \"val_losses\": lstm_hist['val_losses'],\n",
    "                \"val_correlations\": lstm_hist['val_correlations']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    json_path = f'{OUTPUT_DIR}/complete_experiment_data_t3.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(experiment_data, f, indent=4)\n",
    "    print(f\"Complete experiment data saved to '{json_path}'\")\n",
    "\n",
    "    # 2. Plotting\n",
    "    epochs_cnn = range(1, len(cnn_hist['train_losses']) + 1)\n",
    "    epochs_lstm = range(1, len(lstm_hist['train_losses']) + 1)\n",
    "\n",
    "    # --- Plot 1: Loss ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_cnn, cnn_hist['train_losses'], label='CNN Train Loss', color='blue', linestyle='--')\n",
    "    plt.plot(epochs_cnn, cnn_hist['val_losses'], label='CNN Val Loss', color='blue')\n",
    "    plt.plot(epochs_lstm, lstm_hist['train_losses'], label='LSTM Train Loss', color='red', linestyle='--')\n",
    "    plt.plot(epochs_lstm, lstm_hist['val_losses'], label='LSTM Val Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # --- Plot 2: Accuracy (Spearman Correlation) ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_cnn, cnn_hist['train_correlations'], label='CNN Train Spearman', color='blue', linestyle='--')\n",
    "    plt.plot(epochs_cnn, cnn_hist['val_correlations'], label='CNN Val Spearman', color='blue')\n",
    "    plt.plot(epochs_lstm, lstm_hist['train_correlations'], label='LSTM Train Spearman', color='red', linestyle='--')\n",
    "    plt.plot(epochs_lstm, lstm_hist['val_correlations'], label='LSTM Val Spearman', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Spearman Correlation')\n",
    "    plt.title('Training and Validation Spearman Correlation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/accuracy_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Plots saved to 'loss_plot.png' and 'accuracy_plot.png'.\")\n",
    "\n",
    "# Execute\n",
    "save_and_plot_results(\n",
    "    cnn_history, lstm_history, \n",
    "    best_cnn_config, best_lstm_config, \n",
    "    (cnn_test_loss, cnn_test_corr), \n",
    "    (lstm_test_loss, lstm_test_corr)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
