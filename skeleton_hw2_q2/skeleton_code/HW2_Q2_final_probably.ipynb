{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbXSD0GH3RfN"
      },
      "source": [
        "# Deep Learning Homework 2 - Question 2\n",
        "## RNA Binding Protein (RBP) Interaction Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ma8O0mu3RfO"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ao6tSGgx3RfO"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openpyxl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QjVtMaVQ3RfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6190e6f3-2a36-4678-b3fb-d8e7b4fe7b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Create Output Directory\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/dl_data/Outputs_2.2'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH8WJjw3RfP"
      },
      "source": [
        "## 2. Download Data\n",
        "\n",
        "Download the data files from the Google Drive link provided in the homework:\n",
        "- `norm_data.txt`\n",
        "- `metadata.xlsx`\n",
        "\n",
        "Upload them to Colab or mount your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive"
      ],
      "metadata": {
        "id": "87LmOGd_RDUc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6zMi0qRK5OrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb75a4b8-ccdb-4f19-e200-1d3a9be9b84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kC91wjUR3RfP"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/dl_data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bWdIDwo3RfP"
      },
      "source": [
        "## 3. Configuration and Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1- chat gpt code to make the py files visible and not get a module not found error\n"
      ],
      "metadata": {
        "id": "Vv0vNL-MLDcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Setup Repo Info\n",
        "GIT_TOKEN = \"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\"\n",
        "GIT_USERNAME = \"xOagge\"\n",
        "GIT_REPO = \"DeepLearning_Homework2\"\n",
        "\n",
        "# 2. Force Clone\n",
        "if not os.path.exists(f\"/content/{GIT_REPO}\"):\n",
        "    !git clone https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPO}.git\n",
        "else:\n",
        "    print(\"Repo already exists.\")\n",
        "\n",
        "# 3. Use a different search method to find config.py\n",
        "import subprocess\n",
        "try:\n",
        "    # This searches the whole environment for the file\n",
        "    result = subprocess.check_output(['find', '/content', '-name', 'config.py']).decode('utf-8').strip().split('\\n')\n",
        "    if result and result[0]:\n",
        "        config_dir = os.path.dirname(result[0])\n",
        "        if config_dir not in sys.path:\n",
        "            sys.path.insert(0, config_dir)\n",
        "        print(f\"✅ Found config.py in: {config_dir}\")\n",
        "        os.chdir(config_dir)\n",
        "        print(f\"✅ Current directory: {os.getcwd()}\")\n",
        "        !ls\n",
        "    else:\n",
        "        print(\"❌ config.py was NOT found. Are you sure it is in this repo?\")\n",
        "except:\n",
        "    print(\"❌ Find command failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUPwCjx4KSCb",
        "outputId": "13aa4ecd-e2eb-4388-98a7-f572c64b124b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepLearning_Homework2'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 114 (delta 36), reused 48 (delta 21), pack-reused 46 (from 1)\u001b[K\n",
            "Receiving objects: 100% (114/114), 319.14 MiB | 17.50 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "✅ Found config.py in: /content/DeepLearning_Homework2/skeleton_hw2_q2/skeleton_code\n",
            "✅ Current directory: /content/DeepLearning_Homework2/skeleton_hw2_q2/skeleton_code\n",
            "config.py\n",
            "how-to-use-kaggle-tutorial.md\n",
            "HW2_Q2_1_bestTuning.ipynb\n",
            "HW2_Q2_1.ipynb\n",
            "HW2_Q2_1_RNA_Binding_Prediction.ipynb\n",
            "HW2_Q2_1_t1.ipynb\n",
            "HW2_Q2_1_t2.ipynb\n",
            "HW2_Q2_1_t3.ipynb\n",
            "HW2_Q2_1_t4.ipynb\n",
            "HW2_Q2_1_t5.ipynb\n",
            "HW2_Q2_1_t6.ipynb\n",
            "HW2_Q2_before_long_run.ipynb\n",
            "plot_bestModelsOfTrial_NoSave.py\n",
            "plotCompare_CNN_dr01_02_LSTM_01_03.py\n",
            "plotCompare_CNN_dr02_03_LSTM_01_04.py\n",
            "plot_Corr_funcOf_lr_allgrid.py\n",
            "plot_corr_funcOf_lr_NoOverfittingModels_3d.py\n",
            "README.md\n",
            "utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2- imports"
      ],
      "metadata": {
        "id": "iRhV5f5bLMy8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dEKieqTx3RfP"
      },
      "outputs": [],
      "source": [
        "from config import RNAConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5ebeuKmQ3RfQ"
      },
      "outputs": [],
      "source": [
        "from utils import configure_seed, masked_mse_loss, masked_spearman_correlation\n",
        "\n",
        "configure_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kY3uRe3RfQ"
      },
      "source": [
        "## 4. Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n1mTI2lD3RfQ"
      },
      "outputs": [],
      "source": [
        "from utils import RNACompeteLoader, load_rnacompete_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LIA0znn3RfQ"
      },
      "source": [
        "## 5. Model Definitions\n",
        "\n",
        "### 5.1 CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HG1UcssY3RfQ"
      },
      "outputs": [],
      "source": [
        "class RNABindingCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    1D Convolutional Neural Network for RNA sequence binding prediction.\n",
        "\n",
        "    Architecture:\n",
        "    - 3 convolutional layers with increasing channels (64 -> 128 -> 256)\n",
        "    - Batch normalization after each conv layer\n",
        "    - ReLU activation and dropout for regularization\n",
        "    - Global max + average pooling for richer representation\n",
        "    - 2 fully connected layers for regression output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels=4, seq_length=41, hidden_dim=128, dropout=0.3):\n",
        "        super(RNABindingCNN, self).__init__()\n",
        "        #L_out = ((L_in + 2P - K) / S)  + 1\n",
        "\n",
        "        # Convolutional layers with different kernel sizes to capture various motif lengths\n",
        "        # Padding choices allow to preserve input length through the feature maps\n",
        "        #L_out = 41\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        #L_out = 41\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=7, padding=3)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        #L_out = 41\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=9, padding=4)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Global pooling (both max and average)\n",
        "        # allows to get the strongest value of a given feature for the sequence\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "        # allows to get the average of how much a feature is present throughout the sequence\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 2, hidden_dim)  # *2 for concat of max and avg pool\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: (batch, seq_length, 4)\n",
        "        # Conv1d expects: (batch, channels, seq_length)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Convolutional blocks\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Global pooling\n",
        "        max_pool = self.global_max_pool(x).squeeze(-1)\n",
        "        avg_pool = self.global_avg_pool(x).squeeze(-1)\n",
        "        x = torch.cat([max_pool, avg_pool], dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBoXj9sR3RfQ"
      },
      "source": [
        "### 5.2 LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xKhMqThC3RfR"
      },
      "outputs": [],
      "source": [
        "class RNABindingLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM for RNA sequence binding prediction.\n",
        "\n",
        "    Architecture:\n",
        "    - 2-layer bidirectional LSTM\n",
        "    - Batch normalization\n",
        "    - Dropout for regularization\n",
        "    - 2 fully connected layers for regression\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=4, hidden_dim=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super(RNABindingLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim * self.num_directions)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim * self.num_directions, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: (batch, seq_length, 4)\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Concatenate last hidden states from forward and backward\n",
        "            hidden_forward = hidden[-2, :, :]\n",
        "            hidden_backward = hidden[-1, :, :]\n",
        "            combined = torch.cat([hidden_forward, hidden_backward], dim=1)\n",
        "        else:\n",
        "            combined = hidden[-1, :, :]\n",
        "\n",
        "        combined = self.bn(combined)\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        x = self.relu(self.fc1(combined))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.3 LSTM Model with attention (attention is all we need)\n"
      ],
      "metadata": {
        "id": "8Wzkk3FdTtut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNABindingLSTM_attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM for RNA sequence binding prediction.\n",
        "\n",
        "    Architecture:\n",
        "    - 2-layer bidirectional LSTM\n",
        "    - Batch normalization\n",
        "    - Dropout for regularization\n",
        "    - 2 fully connected layers for regression\n",
        "    - 1 scoring layer whose results go through softmax\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=4, hidden_dim=128, num_layers=2, dropout=0.3, bidirectional=True,heads=1):\n",
        "        super(RNABindingLSTM_attention, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        # number of heads will be a new hyperparameter\n",
        "        self.num_heads = heads\n",
        "        # extra layer for judging the value of each hidden state, of size 2*hidden_dim due to bidirectionality\n",
        "        self.layer_attention_result = nn.Linear(hidden_dim * self.num_directions, heads)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim * self.num_directions)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim * self.num_directions, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: (batch, seq_length, 4)\n",
        "        # we will only need lstm_out\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        # now we pass lsmt_out through the new attention layer\n",
        "        # shape of the following scores: [batch, 41, heads]\n",
        "        attention_scores = self.layer_attention_result(lstm_out)\n",
        "        # we get the normalized weights from the scores\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "        # now we get the single vector that\n",
        "        # emcompasses the context from all the hidden states, for each head\n",
        "        # we do the following matrix mutiplication resulting in the weighted\n",
        "        # sum of the scores for each head's weights. weights : [batch, 41, heads]\n",
        "        full_info_per_head = torch.matmul(attention_weights.transpose(1, 2), lstm_out)\n",
        "        # average of the results of each head\n",
        "        final_info = torch.mean(full_info_per_head, dim=1)\n",
        "\n",
        "        combined = self.bn(final_info)\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        x = self.relu(self.fc1(combined))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "beO8fQDVTwEq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxGTwukW3RfR"
      },
      "source": [
        "## 6. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "F3Qx6YCy3RfR"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, device):\n",
        "    \"\"\"Train for one epoch and return loss and correlation.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_masks = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # we make sure this portion of code works for models both with and without attention\n",
        "        output = model(x)\n",
        "        if isinstance(output, tuple):\n",
        "          predictions, attention_weights = output\n",
        "        else:\n",
        "          predictions = output\n",
        "          attention_weights = None\n",
        "        loss = masked_mse_loss(predictions, y, mask)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Collect for metric calculation\n",
        "        all_preds.append(predictions.detach().cpu())\n",
        "        all_targets.append(y.detach().cpu())\n",
        "        all_masks.append(mask.detach().cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "    all_masks = torch.cat(all_masks, dim=0)\n",
        "\n",
        "    spearman_corr = masked_spearman_correlation(all_preds, all_targets, all_masks)\n",
        "\n",
        "    return total_loss / num_batches, spearman_corr.item()\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    \"\"\"Evaluate model and return loss and Spearman correlation.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            x, y, mask = batch\n",
        "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "\n",
        "            output = model(x)\n",
        "\n",
        "# we make sure this portion of code works for models both with and without attention\n",
        "            if isinstance(output, tuple):\n",
        "              predictions, attention_weights = output\n",
        "            else:\n",
        "              predictions = output\n",
        "              attention_weights = None\n",
        "            loss = masked_mse_loss(predictions, y, mask)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            all_preds.append(predictions.cpu())\n",
        "            all_targets.append(y.cpu())\n",
        "            all_masks.append(mask.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "    all_masks = torch.cat(all_masks, dim=0)\n",
        "\n",
        "    spearman_corr = masked_spearman_correlation(all_preds, all_targets, all_masks)\n",
        "\n",
        "    return total_loss / num_batches, spearman_corr.item()\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, device,\n",
        "                num_epochs, model_name, patience=15, save_every=10):\n",
        "    \"\"\"Full training loop with early stopping and periodic checkpoints.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    train_losses = []\n",
        "    train_correlations = []\n",
        "    val_losses = []\n",
        "    val_correlations = []\n",
        "\n",
        "    best_val_corr = -float('inf')\n",
        "    best_model_state = None\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_corr = train_epoch(model, train_loader, optimizer, device)\n",
        "        val_loss, val_corr = evaluate(model, val_loader, device)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_corr)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_correlations.append(train_corr)\n",
        "        val_losses.append(val_loss)\n",
        "        val_correlations.append(val_corr)\n",
        "\n",
        "        if val_corr > best_val_corr:\n",
        "            best_val_corr = val_corr\n",
        "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            epochs_without_improvement = 0\n",
        "            # Save best model to Output directory\n",
        "            torch.save(best_model_state, f'{OUTPUT_DIR}/{model_name}_best.pth')\n",
        "            print(f\"  → Saved new best model (Spearman: {val_corr:.4f})\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} | \"\n",
        "                  f\"Train Spearman: {train_corr:.4f} | \"\n",
        "                  f\"Val Spearman: {val_corr:.4f} | \"\n",
        "                  f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Periodic checkpoint every N epochs\n",
        "        if (epoch + 1) % save_every == 0:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_losses': train_losses,\n",
        "                'train_correlations': train_correlations,\n",
        "                'val_losses': val_losses,\n",
        "                'val_correlations': val_correlations,\n",
        "                'best_val_corr': best_val_corr\n",
        "            }\n",
        "            torch.save(checkpoint, f'{OUTPUT_DIR}/{model_name}_checkpoint_epoch{epoch+1}.pth')\n",
        "            print(f\"  → Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
        "    print(f\"Best validation Spearman correlation: {best_val_corr:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'train_correlations': train_correlations,\n",
        "        'val_losses': val_losses,\n",
        "        'val_correlations': val_correlations,\n",
        "        'best_val_corr': best_val_corr,\n",
        "        'training_time': total_time\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvPBHoqu3RfR"
      },
      "source": [
        "## 7. Load Data\n",
        "###used chat gpt to not get the file not found error, identifying the correct path to the desired files inside google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LG_WFqWc3RfR",
        "outputId": "a25ca3a8-f9d4-481f-e695-171624e871eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data for protein: RBFOX1\n",
            "Loading Metadata from metadata.xlsx...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  > Metadata loaded in 0.66 seconds.\n",
            "Loading Data from norm_data.txt...\n",
            "  > Data Matrix loaded in 48.59 seconds.\n",
            "Saving processed data to data/RBFOX1_train_data.pt...\n",
            "Loading Metadata from metadata.xlsx...\n",
            "  > Metadata loaded in 0.09 seconds.\n",
            "Loading Data from norm_data.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  > Data Matrix loaded in 50.62 seconds.\n",
            "Saving processed data to data/RBFOX1_val_data.pt...\n",
            "Loading Metadata from metadata.xlsx...\n",
            "  > Metadata loaded in 0.05 seconds.\n",
            "Loading Data from norm_data.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  > Data Matrix loaded in 51.36 seconds.\n",
            "Saving processed data to data/RBFOX1_test_data.pt...\n",
            "\n",
            "✅ Success! Dataset sizes:\n",
            "  Train: 96261\n",
            "  Val: 24065\n",
            "  Test: 121031\n"
          ]
        }
      ],
      "source": [
        "# 1. Copy the two files directly from your Drive folder to the local Colab folder\n",
        "!cp \"/content/drive/MyDrive/dl_data/metadata.xlsx\" \"metadata.xlsx\"\n",
        "!cp \"/content/drive/MyDrive/dl_data/norm_data.txt\" \"norm_data.txt\"\n",
        "\n",
        "# 2. Standard loading setup\n",
        "PROTEIN_NAME = 'RBFOX1'\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "print(f\"Loading data for protein: {PROTEIN_NAME}\")\n",
        "config = RNAConfig()\n",
        "\n",
        "# Since the files are now in the local folder, we set these to '.' (current directory)\n",
        "config.data_dir = '.'\n",
        "config.metadata_path = 'metadata.xlsx'\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = load_rnacompete_data(PROTEIN_NAME, split='train', config=config)\n",
        "val_dataset = load_rnacompete_data(PROTEIN_NAME, split='val', config=config)\n",
        "test_dataset = load_rnacompete_data(PROTEIN_NAME, split='test', config=config)\n",
        "\n",
        "print(f\"\\n✅ Success! Dataset sizes:\")\n",
        "print(f\"  Train: {len(train_dataset)}\")\n",
        "print(f\"  Val: {len(val_dataset)}\")\n",
        "print(f\"  Test: {len(test_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XflxQ1VL3RfR"
      },
      "source": [
        "## 8. Hyperparameter Search and Training\n",
        "\n",
        "Here we define the search space and a function to iterate through hyperparameters, training the model for each combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "R5aoUFY08thd"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_search(model_class, model_name, param_grid, train_loader, val_loader, device, epochs=50):\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    best_val_corr = -float('inf')\n",
        "    best_config = None\n",
        "    best_history = None\n",
        "\n",
        "    print(f\"Starting Hyperparameter Search for {model_name} with {len(experiments)} configurations...\")\n",
        "\n",
        "    results = []\n",
        "    # Define a path for the intermediate results\n",
        "    results_path = f'{OUTPUT_DIR}/{model_name}_search_results_2.2.json'\n",
        "\n",
        "    for i, config in enumerate(experiments):\n",
        "        print(f\"\\nRunning experiment {i+1}/{len(experiments)}: {config}\")\n",
        "\n",
        "        configure_seed(42)\n",
        "\n",
        "        # Initialize model with current config\n",
        "        if model_name == 'LSTM':\n",
        "             model = model_class(\n",
        "                hidden_dim=config['hidden_dim'],\n",
        "                dropout=config['dropout']\n",
        "            ).to(device)\n",
        "        elif model_name == 'Attention_LSTM':\n",
        "             model = model_class(\n",
        "                hidden_dim=config['hidden_dim'],\n",
        "                dropout=config['dropout'],\n",
        "                heads=config.get('heads', 1)\n",
        "            ).to(device)\n",
        "        else: # CNN case\n",
        "             model = model_class(\n",
        "                hidden_dim=config['hidden_dim'],\n",
        "                dropout=config['dropout']\n",
        "            ).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "        # temp_name will be e.g. \"CNN_exp0\"\n",
        "        temp_name = f\"{model_name}_exp{i}\"\n",
        "\n",
        "        # Train - the trainer saves a file named: {OUTPUT_DIR}/{temp_name}_best.pth\n",
        "        history = train_model(\n",
        "            model, train_loader, val_loader, optimizer, scheduler,\n",
        "            device, num_epochs=epochs, model_name=temp_name, patience=7, save_every=100\n",
        "        )\n",
        "\n",
        "        # SAVE PROGRESS TO JSON\n",
        "        run_result = {\n",
        "            'experiment_index': i,\n",
        "            'config': config,\n",
        "            'best_val_corr': history['best_val_corr'],\n",
        "            'training_time': history['training_time'],\n",
        "            'history': history\n",
        "        }\n",
        "        results.append(run_result)\n",
        "\n",
        "        try:\n",
        "            with open(results_path, 'w') as f:\n",
        "                json.dump(results, f, indent=4)\n",
        "        except Exception as e:\n",
        "            print(f\"  → Warning: Could not save progress to JSON: {e}\")\n",
        "\n",
        "        # Check if this is the best result so far\n",
        "        if history['best_val_corr'] > best_val_corr:\n",
        "            best_val_corr = history['best_val_corr']\n",
        "            best_config = config\n",
        "            best_history = history\n",
        "            actual_saved_file = f'{OUTPUT_DIR}/{temp_name}_best.pth'\n",
        "            #verification that the file was properly created\n",
        "            if os.path.exists(actual_saved_file):\n",
        "                best_state = torch.load(actual_saved_file)\n",
        "                # Save for plotting later on\n",
        "                torch.save(best_state, f'{OUTPUT_DIR}/{model_name}_best.pth')\n",
        "                # Save in the final naming format\n",
        "                torch.save(best_state, f'{OUTPUT_DIR}/best_{model_name.lower()}_model.pth')\n",
        "                print(f\" Updated best model for {model_name}\")\n",
        "            else:\n",
        "                print(f\" Error because the checkpoint wasnt found: {actual_saved_file}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Best {model_name} configuration: {best_config}\")\n",
        "    print(f\"Best validation Spearman: {best_val_corr:.4f}\")\n",
        "    print(f\"Best model saved to {OUTPUT_DIR}/{model_name}_best.pth\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return best_config, best_history\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GRIds for the grid search\n",
        "cnn_param_grid = {\n",
        "    'learning_rate': [0.007, 0.009],\n",
        "    'hidden_dim': [512],\n",
        "    'dropout': [0.2, 0.3]\n",
        "}\n",
        "\n",
        "\n",
        "lstm_param_grid = {\n",
        "    'learning_rate': [0.0005, 0.0007],\n",
        "    'hidden_dim': [512],\n",
        "    'dropout': [0.3, 0.4]\n",
        "}\n",
        "\n",
        "attention_lstm_param_grid = {\n",
        "    'learning_rate': [0.0003, 0.0005],\n",
        "    'hidden_dim': [512],\n",
        "    'dropout': [0.3],\n",
        "    'heads': [1, 2]\n",
        "}"
      ],
      "metadata": {
        "id": "4ImzFA-QTIzm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1Z6ovq-_8the",
        "outputId": "4e15a70a-7575-423e-d774-7f070eeddd17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Tuning CNN ----\n",
            "Starting Hyperparameter Search for CNN with 4 configurations...\n",
            "\n",
            "Running experiment 1/4: {'learning_rate': 0.007, 'hidden_dim': 512, 'dropout': 0.2}\n",
            "\n",
            "============================================================\n",
            "Training CNN_exp0\n",
            "============================================================\n",
            "  → Saved new best model (Spearman: 0.5174)\n",
            "Epoch   1/50 | Train Loss: 1.5079 | Val Loss: 0.6988 | Train Spearman: 0.4325 | Val Spearman: 0.5174 | Time: 19.21s\n",
            "  → Saved new best model (Spearman: 0.5769)\n",
            "  → Saved new best model (Spearman: 0.5890)\n",
            "  → Saved new best model (Spearman: 0.6014)\n",
            "  → Saved new best model (Spearman: 0.6051)\n",
            "Epoch   5/50 | Train Loss: 0.4604 | Val Loss: 0.4320 | Train Spearman: 0.5524 | Val Spearman: 0.6051 | Time: 18.89s\n",
            "  → Saved new best model (Spearman: 0.6157)\n",
            "  → Saved new best model (Spearman: 0.6174)\n",
            "Epoch  10/50 | Train Loss: 0.4359 | Val Loss: 0.4151 | Train Spearman: 0.5704 | Val Spearman: 0.5976 | Time: 18.89s\n",
            "  → Saved new best model (Spearman: 0.6234)\n",
            "Epoch  15/50 | Train Loss: 0.4345 | Val Loss: 0.3885 | Train Spearman: 0.5735 | Val Spearman: 0.6183 | Time: 18.77s\n",
            "  → Saved new best model (Spearman: 0.6297)\n",
            "  → Saved new best model (Spearman: 0.6301)\n",
            "Epoch  20/50 | Train Loss: 0.4261 | Val Loss: 0.4135 | Train Spearman: 0.5805 | Val Spearman: 0.6301 | Time: 18.70s\n",
            "  → Saved new best model (Spearman: 0.6305)\n",
            "Epoch  25/50 | Train Loss: 0.4183 | Val Loss: 0.4411 | Train Spearman: 0.5853 | Val Spearman: 0.6167 | Time: 18.88s\n",
            "  → Saved new best model (Spearman: 0.6341)\n",
            "Epoch  30/50 | Train Loss: 0.4175 | Val Loss: 0.4074 | Train Spearman: 0.5837 | Val Spearman: 0.6242 | Time: 18.87s\n",
            "  → Saved new best model (Spearman: 0.6476)\n",
            "Epoch  35/50 | Train Loss: 0.3825 | Val Loss: 0.3894 | Train Spearman: 0.6058 | Val Spearman: 0.6372 | Time: 18.70s\n",
            "  → Saved new best model (Spearman: 0.6480)\n",
            "Epoch  40/50 | Train Loss: 0.3707 | Val Loss: 0.3624 | Train Spearman: 0.6119 | Val Spearman: 0.6432 | Time: 18.54s\n",
            "  → Saved new best model (Spearman: 0.6495)\n",
            "Epoch  45/50 | Train Loss: 0.3628 | Val Loss: 0.3676 | Train Spearman: 0.6181 | Val Spearman: 0.6476 | Time: 18.51s\n",
            "  → Saved new best model (Spearman: 0.6534)\n",
            "  → Saved new best model (Spearman: 0.6580)\n",
            "Epoch  50/50 | Train Loss: 0.3437 | Val Loss: 0.3698 | Train Spearman: 0.6270 | Val Spearman: 0.6580 | Time: 18.94s\n",
            "\n",
            "Training completed in 938.81s\n",
            "Best validation Spearman correlation: 0.6580\n",
            " Updated best model for CNN\n",
            "\n",
            "Running experiment 2/4: {'learning_rate': 0.007, 'hidden_dim': 512, 'dropout': 0.3}\n",
            "\n",
            "============================================================\n",
            "Training CNN_exp1\n",
            "============================================================\n",
            "  → Saved new best model (Spearman: 0.5226)\n",
            "Epoch   1/50 | Train Loss: 1.8313 | Val Loss: 0.5477 | Train Spearman: 0.4015 | Val Spearman: 0.5226 | Time: 19.03s\n",
            "  → Saved new best model (Spearman: 0.5513)\n",
            "  → Saved new best model (Spearman: 0.5618)\n",
            "  → Saved new best model (Spearman: 0.5702)\n",
            "  → Saved new best model (Spearman: 0.5831)\n",
            "Epoch   5/50 | Train Loss: 0.5093 | Val Loss: 0.4753 | Train Spearman: 0.5221 | Val Spearman: 0.5831 | Time: 19.03s\n",
            "  → Saved new best model (Spearman: 0.6032)\n",
            "  → Saved new best model (Spearman: 0.6051)\n",
            "Epoch  10/50 | Train Loss: 0.4764 | Val Loss: 0.4138 | Train Spearman: 0.5469 | Val Spearman: 0.6002 | Time: 18.62s\n",
            "  → Saved new best model (Spearman: 0.6106)\n",
            "  → Saved new best model (Spearman: 0.6189)\n",
            "Epoch  15/50 | Train Loss: 0.4695 | Val Loss: 0.4623 | Train Spearman: 0.5532 | Val Spearman: 0.6107 | Time: 18.53s\n",
            "Epoch  20/50 | Train Loss: 0.4607 | Val Loss: 0.4682 | Train Spearman: 0.5551 | Val Spearman: 0.6156 | Time: 18.50s\n",
            "  → Saved new best model (Spearman: 0.6289)\n",
            "  → Saved new best model (Spearman: 0.6296)\n",
            "Epoch  25/50 | Train Loss: 0.4181 | Val Loss: 0.4086 | Train Spearman: 0.5848 | Val Spearman: 0.6296 | Time: 18.88s\n",
            "  → Saved new best model (Spearman: 0.6357)\n",
            "Epoch  30/50 | Train Loss: 0.4115 | Val Loss: 0.3910 | Train Spearman: 0.5879 | Val Spearman: 0.6342 | Time: 18.78s\n",
            "  → Saved new best model (Spearman: 0.6395)\n",
            "Epoch  35/50 | Train Loss: 0.4053 | Val Loss: 0.4323 | Train Spearman: 0.5890 | Val Spearman: 0.6381 | Time: 18.67s\n",
            "Epoch  40/50 | Train Loss: 0.4077 | Val Loss: 0.3906 | Train Spearman: 0.5880 | Val Spearman: 0.6370 | Time: 18.60s\n",
            "  → Saved new best model (Spearman: 0.6427)\n",
            "  → Saved new best model (Spearman: 0.6453)\n",
            "Epoch  45/50 | Train Loss: 0.3794 | Val Loss: 0.3857 | Train Spearman: 0.6054 | Val Spearman: 0.6453 | Time: 18.55s\n",
            "  → Saved new best model (Spearman: 0.6477)\n",
            "Epoch  50/50 | Train Loss: 0.3765 | Val Loss: 0.4001 | Train Spearman: 0.6087 | Val Spearman: 0.6477 | Time: 18.91s\n",
            "\n",
            "Training completed in 938.00s\n",
            "Best validation Spearman correlation: 0.6477\n",
            "\n",
            "Running experiment 3/4: {'learning_rate': 0.009, 'hidden_dim': 512, 'dropout': 0.2}\n",
            "\n",
            "============================================================\n",
            "Training CNN_exp2\n",
            "============================================================\n",
            "  → Saved new best model (Spearman: 0.5308)\n",
            "Epoch   1/50 | Train Loss: 2.2248 | Val Loss: 0.4972 | Train Spearman: 0.4283 | Val Spearman: 0.5308 | Time: 18.95s\n",
            "  → Saved new best model (Spearman: 0.5690)\n",
            "  → Saved new best model (Spearman: 0.5778)\n",
            "  → Saved new best model (Spearman: 0.5892)\n",
            "  → Saved new best model (Spearman: 0.5927)\n",
            "Epoch   5/50 | Train Loss: 0.4790 | Val Loss: 0.4821 | Train Spearman: 0.5426 | Val Spearman: 0.5927 | Time: 19.02s\n",
            "  → Saved new best model (Spearman: 0.6096)\n",
            "Epoch  10/50 | Train Loss: 0.4532 | Val Loss: 0.3985 | Train Spearman: 0.5600 | Val Spearman: 0.5980 | Time: 18.70s\n",
            "  → Saved new best model (Spearman: 0.6114)\n",
            "  → Saved new best model (Spearman: 0.6189)\n",
            "Epoch  15/50 | Train Loss: 0.4486 | Val Loss: 0.4379 | Train Spearman: 0.5665 | Val Spearman: 0.6189 | Time: 18.54s\n",
            "  → Saved new best model (Spearman: 0.6236)\n",
            "Epoch  20/50 | Train Loss: 0.4482 | Val Loss: 0.4358 | Train Spearman: 0.5654 | Val Spearman: 0.6109 | Time: 18.54s\n",
            "  → Saved new best model (Spearman: 0.6300)\n",
            "Epoch  25/50 | Train Loss: 0.4431 | Val Loss: 0.4852 | Train Spearman: 0.5677 | Val Spearman: 0.5894 | Time: 18.95s\n",
            "  → Saved new best model (Spearman: 0.6332)\n",
            "Epoch  30/50 | Train Loss: 0.4031 | Val Loss: 0.3759 | Train Spearman: 0.5921 | Val Spearman: 0.6287 | Time: 18.75s\n",
            "  → Saved new best model (Spearman: 0.6377)\n",
            "  → Saved new best model (Spearman: 0.6430)\n",
            "Epoch  35/50 | Train Loss: 0.3890 | Val Loss: 0.4117 | Train Spearman: 0.5980 | Val Spearman: 0.6403 | Time: 18.64s\n",
            "  → Saved new best model (Spearman: 0.6463)\n",
            "Epoch  40/50 | Train Loss: 0.3913 | Val Loss: 0.3732 | Train Spearman: 0.6010 | Val Spearman: 0.6420 | Time: 18.47s\n",
            "\n",
            "Early stopping at epoch 44\n",
            "\n",
            "Training completed in 823.16s\n",
            "Best validation Spearman correlation: 0.6463\n",
            "\n",
            "Running experiment 4/4: {'learning_rate': 0.009, 'hidden_dim': 512, 'dropout': 0.3}\n",
            "\n",
            "============================================================\n",
            "Training CNN_exp3\n",
            "============================================================\n",
            "  → Saved new best model (Spearman: 0.5270)\n",
            "Epoch   1/50 | Train Loss: 2.8444 | Val Loss: 0.5993 | Train Spearman: 0.4113 | Val Spearman: 0.5270 | Time: 19.06s\n",
            "  → Saved new best model (Spearman: 0.5588)\n",
            "  → Saved new best model (Spearman: 0.5686)\n",
            "  → Saved new best model (Spearman: 0.5780)\n",
            "Epoch   5/50 | Train Loss: 0.5160 | Val Loss: 0.5541 | Train Spearman: 0.5132 | Val Spearman: 0.5780 | Time: 18.91s\n",
            "  → Saved new best model (Spearman: 0.5885)\n",
            "  → Saved new best model (Spearman: 0.5951)\n",
            "Epoch  10/50 | Train Loss: 0.4881 | Val Loss: 0.4301 | Train Spearman: 0.5300 | Val Spearman: 0.5951 | Time: 18.69s\n",
            "  → Saved new best model (Spearman: 0.6012)\n",
            "Epoch  15/50 | Train Loss: 0.4928 | Val Loss: 0.4706 | Train Spearman: 0.5337 | Val Spearman: 0.6012 | Time: 19.02s\n",
            "  → Saved new best model (Spearman: 0.6016)\n",
            "Epoch  20/50 | Train Loss: 0.4774 | Val Loss: 0.5208 | Train Spearman: 0.5355 | Val Spearman: 0.5973 | Time: 19.05s\n",
            "  → Saved new best model (Spearman: 0.6048)\n",
            "  → Saved new best model (Spearman: 0.6104)\n",
            "  → Saved new best model (Spearman: 0.6160)\n",
            "Epoch  25/50 | Train Loss: 0.4405 | Val Loss: 0.4263 | Train Spearman: 0.5645 | Val Spearman: 0.6160 | Time: 18.55s\n",
            "  → Saved new best model (Spearman: 0.6240)\n",
            "  → Saved new best model (Spearman: 0.6251)\n",
            "Epoch  30/50 | Train Loss: 0.4307 | Val Loss: 0.4602 | Train Spearman: 0.5753 | Val Spearman: 0.6205 | Time: 18.55s\n",
            "  → Saved new best model (Spearman: 0.6251)\n",
            "  → Saved new best model (Spearman: 0.6254)\n",
            "  → Saved new best model (Spearman: 0.6274)\n",
            "Epoch  35/50 | Train Loss: 0.4277 | Val Loss: 0.4463 | Train Spearman: 0.5705 | Val Spearman: 0.6274 | Time: 18.66s\n",
            "  → Saved new best model (Spearman: 0.6295)\n",
            "  → Saved new best model (Spearman: 0.6332)\n",
            "Epoch  40/50 | Train Loss: 0.4263 | Val Loss: 0.3884 | Train Spearman: 0.5731 | Val Spearman: 0.6250 | Time: 19.11s\n",
            "Epoch  45/50 | Train Loss: 0.4190 | Val Loss: 0.4689 | Train Spearman: 0.5800 | Val Spearman: 0.6107 | Time: 18.89s\n",
            "  → Saved new best model (Spearman: 0.6358)\n",
            "  → Saved new best model (Spearman: 0.6386)\n",
            "Epoch  50/50 | Train Loss: 0.3881 | Val Loss: 0.4273 | Train Spearman: 0.5935 | Val Spearman: 0.6386 | Time: 19.38s\n",
            "\n",
            "Training completed in 944.14s\n",
            "Best validation Spearman correlation: 0.6386\n",
            "\n",
            "============================================================\n",
            "Best CNN configuration: {'learning_rate': 0.007, 'hidden_dim': 512, 'dropout': 0.2}\n",
            "Best validation Spearman: 0.6580\n",
            "Best model saved to /content/drive/MyDrive/dl_data/Outputs_2.2/CNN_best.pth\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 8.1 Train and Search CNN\n",
        "print(\"---- Tuning CNN ----\")\n",
        "best_cnn_config, cnn_history = hyperparameter_search(\n",
        "    RNABindingCNN, 'CNN', cnn_param_grid, train_loader, val_loader, device, epochs=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "91pB0Lgz8thf",
        "outputId": "b6dc7002-d470-4dbf-e292-6cb8e53590b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Tuning LSTM ----\n",
            "Starting Hyperparameter Search for LSTM with 4 configurations...\n",
            "\n",
            "Running experiment 1/4: {'learning_rate': 0.0005, 'hidden_dim': 512, 'dropout': 0.3}\n",
            "\n",
            "============================================================\n",
            "Training LSTM_exp0\n",
            "============================================================\n",
            "  → Saved new best model (Spearman: 0.3417)\n",
            "Epoch   1/50 | Train Loss: 0.9567 | Val Loss: 2.3679 | Train Spearman: 0.2501 | Val Spearman: 0.3417 | Time: 91.67s\n",
            "  → Saved new best model (Spearman: 0.4740)\n",
            "  → Saved new best model (Spearman: 0.5256)\n",
            "Epoch   5/50 | Train Loss: 0.4970 | Val Loss: 0.5442 | Train Spearman: 0.5436 | Val Spearman: 0.5052 | Time: 90.69s\n",
            "  → Saved new best model (Spearman: 0.5627)\n",
            "  → Saved new best model (Spearman: 0.6054)\n",
            "  → Saved new best model (Spearman: 0.6251)\n",
            "  → Saved new best model (Spearman: 0.6262)\n",
            "Epoch  10/50 | Train Loss: 0.4018 | Val Loss: 0.3795 | Train Spearman: 0.6051 | Val Spearman: 0.6262 | Time: 91.73s\n",
            "  → Saved new best model (Spearman: 0.6327)\n",
            "  → Saved new best model (Spearman: 0.6372)\n",
            "  → Saved new best model (Spearman: 0.6381)\n",
            "  → Saved new best model (Spearman: 0.6451)\n",
            "Epoch  15/50 | Train Loss: 0.3638 | Val Loss: 0.4197 | Train Spearman: 0.6302 | Val Spearman: 0.6451 | Time: 90.98s\n",
            "  → Saved new best model (Spearman: 0.6452)\n"
          ]
        }
      ],
      "source": [
        "# 8.2 Train and Search LSTM\n",
        "print(\"---- Tuning LSTM ----\")\n",
        "best_lstm_config, lstm_history = hyperparameter_search(\n",
        "    RNABindingLSTM, 'LSTM', lstm_param_grid, train_loader, val_loader, device, epochs=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.3 Train and Search Attention LSTM (Question 2.2)\n",
        "print(\"---- Tuning Attention LSTM ----\")\n",
        "best_attention_config, attention_history = hyperparameter_search(\n",
        "    RNABindingLSTM_attention,\n",
        "    'Attention_LSTM',\n",
        "    attention_lstm_param_grid,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    epochs=50\n",
        ")"
      ],
      "metadata": {
        "id": "Y66zfy7mHzKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlEimsfj3RfR"
      },
      "source": [
        "## 9. Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax4WTYYe3RfR",
        "outputId": "2e465136-1b34-46cb-b616-9a48b780fe28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best CNN Test Results:\n",
            "  Test Loss: 0.3633\n",
            "  Test Spearman Correlation: 0.6601\n"
          ]
        }
      ],
      "source": [
        "# Load best CNN model\n",
        "cnn_model = RNABindingCNN(\n",
        "    hidden_dim=best_cnn_config['hidden_dim'],\n",
        "    dropout=best_cnn_config['dropout']\n",
        ").to(device)\n",
        "cnn_model.load_state_dict(torch.load(f'{OUTPUT_DIR}/best_cnn_model.pth'))\n",
        "\n",
        "# evaluate CNN on test set\n",
        "cnn_test_res = evaluate(cnn_model, test_loader, device)\n",
        "cnn_test_loss, cnn_test_corr = cnn_test_res\n",
        "\n",
        "print(f\"\\nBest CNN Test Results:\")\n",
        "print(f\"  Test Loss: {cnn_test_loss:.4f}\")\n",
        "print(f\"  Test Spearman Correlation: {cnn_test_corr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKvfoFQ53RfR",
        "outputId": "a5304b81-c745-49ef-93da-295deabd9c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best LSTM Test Results:\n",
            "  Test Loss: 0.3423\n",
            "  Test Spearman Correlation: 0.6576\n"
          ]
        }
      ],
      "source": [
        "# Load best LSTM model\n",
        "lstm_model = RNABindingLSTM(\n",
        "    hidden_dim=best_lstm_config['hidden_dim'],\n",
        "    dropout=best_lstm_config['dropout']\n",
        ").to(device)\n",
        "lstm_model.load_state_dict(torch.load(f'{OUTPUT_DIR}/best_lstm_model.pth'))\n",
        "\n",
        "# evaluate LSTM on test set\n",
        "lstm_test_res = evaluate(lstm_model, test_loader, device)\n",
        "lstm_test_loss, lstm_test_corr = lstm_test_res\n",
        "\n",
        "print(f\"\\nBest LSTM Test Results:\")\n",
        "print(f\"  Test Loss: {lstm_test_loss:.4f}\")\n",
        "print(f\"  Test Spearman Correlation: {lstm_test_corr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best Attention LSTM model\n",
        "attention_model = RNABindingLSTM_attention(\n",
        "    hidden_dim=best_attention_config['hidden_dim'],\n",
        "    dropout=best_attention_config['dropout'],\n",
        "    heads=best_attention_config.get('heads', 1)\n",
        ").to(device)\n",
        "attention_model.load_state_dict(torch.load(f'{OUTPUT_DIR}/best_attention_lstm_model.pth'))\n",
        "\n",
        "# Evaluate Attention LSTM on test set\n",
        "attention_test_res = evaluate(attention_model, test_loader, device)\n",
        "attention_test_loss, attention_test_corr = attention_test_res\n",
        "\n",
        "print(f\"\\nBest Attention LSTM Test Results:\")\n",
        "print(f\"  Test Loss: {attention_test_loss:.4f}\")\n",
        "print(f\"  Test Spearman Correlation: {attention_test_corr:.4f}\")"
      ],
      "metadata": {
        "id": "M-WRbPu2JDjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0hVYj_I3RfR"
      },
      "source": [
        "## 10. Plotting Results and Saving History\n",
        "\n",
        "Saving plots and full experiment history to JSON for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxyLw6-33RfR",
        "outputId": "59370eb3-15e6-444f-9a35-b1a554218fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete experiment data saved to 'Outputs_t3/complete_experiment_data_t3.json'\n",
            "Plots saved to 'loss_plot.png' and 'accuracy_plot.png'.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def save_and_plot_results(cnn_hist, lstm_hist, attention_hist, cnn_config, lstm_config, attention_config,\n",
        "                          cnn_test_res, lstm_test_res, attention_test_res):\n",
        "    # Unpack test results\n",
        "    cnn_test_loss, cnn_test_corr = cnn_test_res\n",
        "    lstm_test_loss, lstm_test_corr = lstm_test_res\n",
        "    attention_test_loss, attention_test_corr = attention_test_res\n",
        "    # 1. Prepare Data for JSON Storage\n",
        "    experiment_data = {\n",
        "        \"CNN\": {\n",
        "            \"parameters\": cnn_config,\n",
        "            \"training_time_seconds\": cnn_hist['training_time'],\n",
        "            \"best_val_accuracy\": cnn_hist['best_val_corr'],\n",
        "            \"best_val_epoch\": int(np.argmax(cnn_hist['val_correlations']) + 1),\n",
        "            \"test_loss\": cnn_test_loss,\n",
        "            \"test_accuracy\": cnn_test_corr,\n",
        "            \"history\": {\n",
        "                \"train_losses\": cnn_hist['train_losses'],\n",
        "                \"train_correlations\": cnn_hist['train_correlations'],\n",
        "                \"val_losses\": cnn_hist['val_losses'],\n",
        "                \"val_correlations\": cnn_hist['val_correlations']\n",
        "            }\n",
        "        },\n",
        "        \"LSTM\": {\n",
        "            \"parameters\": lstm_config,\n",
        "            \"training_time_seconds\": lstm_hist['training_time'],\n",
        "            \"best_val_accuracy\": lstm_hist['best_val_corr'],\n",
        "            \"best_val_epoch\": int(np.argmax(lstm_hist['val_correlations']) + 1),\n",
        "            \"test_loss\": lstm_test_loss,\n",
        "            \"test_accuracy\": lstm_test_corr,\n",
        "            \"history\": {\n",
        "                \"train_losses\": lstm_hist['train_losses'],\n",
        "                \"train_correlations\": lstm_hist['train_correlations'],\n",
        "                \"val_losses\": lstm_hist['val_losses'],\n",
        "                \"val_correlations\": lstm_hist['val_correlations']\n",
        "            }\n",
        "        },\n",
        "        \"Attention_LSTM\": {\n",
        "            \"parameters\": attention_config,\n",
        "            \"training_time_seconds\": attention_hist['training_time'],\n",
        "            \"best_val_accuracy\": attention_hist['best_val_corr'],\n",
        "            \"best_val_epoch\": int(np.argmax(attention_hist['val_correlations']) + 1),\n",
        "            \"test_loss\": attention_test_loss,\n",
        "            \"test_accuracy\": attention_test_corr,\n",
        "            \"history\": attention_hist\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save to JSON\n",
        "    json_path = f'{OUTPUT_DIR}/complete_experiment_data_final.json'\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(experiment_data, f, indent=4)\n",
        "    print(f\"Complete experiment data saved to '{json_path}'\")\n",
        "\n",
        "    # 2. Plotting\n",
        "    epochs_cnn = range(1, len(cnn_hist['train_losses']) + 1)\n",
        "    epochs_lstm = range(1, len(lstm_hist['train_losses']) + 1)\n",
        "    epochs_attention = range(1, len(attention_hist['train_losses']) + 1)\n",
        "\n",
        "    # --- Plot 1: Loss ---\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_cnn, cnn_hist['train_losses'], label='CNN Train Loss', color='blue', linestyle='--')\n",
        "    plt.plot(epochs_cnn, cnn_hist['val_losses'], label='CNN Val Loss', color='blue')\n",
        "    plt.plot(epochs_lstm, lstm_hist['train_losses'], label='LSTM Train Loss', color='red', linestyle='--')\n",
        "    plt.plot(epochs_lstm, lstm_hist['val_losses'], label='LSTM Val Loss', color='red')\n",
        "    plt.plot(epochs_attention, attention_hist['val_losses'], label='Attention LSTM Val Loss', color='yellow')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_DIR}/loss_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "    # --- Plot 2: Accuracy (Spearman Correlation) ---\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_cnn, cnn_hist['train_correlations'], label='CNN Train Spearman', color='blue', linestyle='--')\n",
        "    plt.plot(epochs_cnn, cnn_hist['val_correlations'], label='CNN Val Spearman', color='blue')\n",
        "    plt.plot(epochs_lstm, lstm_hist['train_correlations'], label='LSTM Train Spearman', color='red', linestyle='--')\n",
        "    plt.plot(epochs_lstm, lstm_hist['val_correlations'], label='LSTM Val Spearman', color='red')\n",
        "    plt.plot(epochs_attention, attention_hist['val_correlations'], label='Attention LSTM Val Spearman', color='green')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Spearman Correlation')\n",
        "    plt.title('Training and Validation Spearman Correlation')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_DIR}/accuracy_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Plots saved to 'loss_plot.png' and 'accuracy_plot.png'.\")\n",
        "\n",
        "# Execute\n",
        "save_and_plot_results(\n",
        "    cnn_history, lstm_history, attention_history,\n",
        "    best_cnn_config, best_lstm_config, best_attention_config,\n",
        "    cnn_test_res, lstm_test_res, attention_test_res\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11- Ploting attention heatmaps\n",
        "###Adapted chat gpt code to vizualize the behaviour of the attention mechanism and interpret results in the report\n"
      ],
      "metadata": {
        "id": "TefNfWVvNEWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_heatmaps(model, data_loader, device, num_samples=10):\n",
        "    model.eval()\n",
        "    samples_found = 0\n",
        "\n",
        "    bases = ['A', 'C', 'G', 'U']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, mask in data_loader:\n",
        "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "            predictions, attention_weights = model(x)\n",
        "            attention_weights = attention_weights.cpu().numpy()\n",
        "            x_np = x.cpu().numpy()\n",
        "\n",
        "            for i in range(x.size(0)):\n",
        "                if samples_found >= num_samples:\n",
        "                    return\n",
        "\n",
        "                # Convert one-hot back to sequence string for labeling\n",
        "                seq_indices = np.argmax(x_np[i], axis=1)\n",
        "                sequence_str = \"\".join([bases[idx] for idx in seq_indices])\n",
        "\n",
        "                # Plotting\n",
        "                num_heads = attention_weights.shape[2]\n",
        "                plt.figure(figsize=(15, 1 + heads))\n",
        "                # We transpose to have [heads, sequence_length]\n",
        "                sns.heatmap(attention_weights[i].T, annot=False, cmap='YlGnBu',\n",
        "                            xticklabels=list(sequence_str), yticklabels=[f\"Head {j+1}\" for j in range(attention_weights.shape[2])])\n",
        "\n",
        "                plt.title(f\"Sample {samples_found + 1} | Target Affinity: {y[i].item():.3f}\")\n",
        "                plt.xlabel(\"RNA Sequence (5' -> 3')\")\n",
        "                plt.ylabel(\"Attention Heads\")\n",
        "                plt.show()\n",
        "\n",
        "                samples_found += 1\n",
        "\n",
        "# running the vizualization\n",
        "plot_attention_heatmaps(attention_model, test_loader, device, num_samples=6)"
      ],
      "metadata": {
        "id": "CwRAR614NDfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}