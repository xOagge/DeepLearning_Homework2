{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbXSD0GH3RfN"
      },
      "source": [
        "# Deep Learning Homework 2 - Question 2.1\n",
        "## RNA Binding Protein (RBP) Interaction Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ma8O0mu3RfO"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao6tSGgx3RfO"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openpyxl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjVtMaVQ3RfP",
        "outputId": "faaa2cc1-8f18-4e2b-b80e-b7e2ce4e5fdb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH8WJjw3RfP"
      },
      "source": [
        "## 2. Download Data\n",
        "\n",
        "Download the data files from the Google Drive link provided in the homework:\n",
        "- `norm_data.txt`\n",
        "- `metadata.xlsx`\n",
        "\n",
        "Upload them to Colab or mount your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMi0qRK5OrF",
        "outputId": "063d4e7f-b30d-4748-977d-9bdbc13c30aa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvjU8kEE6bkH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC91wjUR3RfP"
      },
      "outputs": [],
      "source": [
        "# Set the path to your data files\n",
        "DATA_DIR = 'MyDrive/path'  # Change this to your folder path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bWdIDwo3RfP"
      },
      "source": [
        "## 3. Configuration and Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEKieqTx3RfP"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class RNAConfig:\n",
        "    \"\"\"Global configuration for the RNAcompete Data Pipeline.\"\"\"\n",
        "\n",
        "    # Data Path - UPDATE THESE PATHS\n",
        "    DATA_PATH: str = f\"{DATA_DIR}/norm_data.txt\"\n",
        "    METADATA_PATH: str = f\"{DATA_DIR}/metadata.xlsx\"\n",
        "    METADATA_SHEET: str = \"Master List--Plasmid Info\"\n",
        "\n",
        "    # Save Path\n",
        "    SAVE_DIR: str = \"data\"\n",
        "\n",
        "    # Sequence Parameters\n",
        "    SEQ_MAX_LEN: int = 41\n",
        "    ALPHABET: str = \"ACGUN\"\n",
        "\n",
        "    # Preprocessing\n",
        "    CLIP_PERCENTILE: float = 99.95\n",
        "    EPSILON: float = 1e-6\n",
        "\n",
        "    # Split Identifiers\n",
        "    TRAIN_SPLIT_ID: str = \"SetA\"\n",
        "    TEST_SPLIT_ID: str = \"SetB\"\n",
        "\n",
        "    VAL_SPLIT_PCT: float = 0.2\n",
        "    SEED: int = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ebeuKmQ3RfQ"
      },
      "outputs": [],
      "source": [
        "def configure_seed(seed):\n",
        "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "configure_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAUiV1Yr3RfQ"
      },
      "outputs": [],
      "source": [
        "def masked_mse_loss(preds, targets, masks):\n",
        "    \"\"\"\n",
        "    Calculates Mean Squared Error, ignoring padded elements.\n",
        "    \"\"\"\n",
        "    preds = preds.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "    masks = masks.squeeze().bool()\n",
        "\n",
        "    masked_preds = preds[masks]\n",
        "    masked_targets = targets[masks]\n",
        "\n",
        "    if masked_preds.numel() == 0:\n",
        "        return torch.tensor(0.0, device=preds.device, requires_grad=True)\n",
        "\n",
        "    squared_error = (masked_preds - masked_targets) ** 2\n",
        "    loss = torch.mean(squared_error)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def masked_spearman_correlation(preds, targets, masks):\n",
        "    \"\"\"\n",
        "    Calculates Spearman Rank Correlation on masked data.\n",
        "    \"\"\"\n",
        "    preds = preds.squeeze().detach()\n",
        "    targets = targets.squeeze().detach()\n",
        "    masks = masks.squeeze().bool()\n",
        "\n",
        "    valid_preds = preds[masks]\n",
        "    valid_targets = targets[masks]\n",
        "\n",
        "    if valid_preds.numel() < 2:\n",
        "        return torch.tensor(0.0)\n",
        "\n",
        "    pred_ranks = valid_preds.argsort().argsort().float()\n",
        "    target_ranks = valid_targets.argsort().argsort().float()\n",
        "\n",
        "    pred_mean = pred_ranks.mean()\n",
        "    target_mean = target_ranks.mean()\n",
        "\n",
        "    pred_var = pred_ranks - pred_mean\n",
        "    target_var = target_ranks - target_mean\n",
        "\n",
        "    correlation = (pred_var * target_var).sum() / torch.sqrt((pred_var ** 2).sum() * (target_var ** 2).sum() + 1e-8)\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kY3uRe3RfQ"
      },
      "source": [
        "## 4. Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1mTI2lD3RfQ"
      },
      "outputs": [],
      "source": [
        "class RNACompeteLoader:\n",
        "    def __init__(self, config: RNAConfig):\n",
        "        self.cfg = config\n",
        "        self.meta_df = None\n",
        "        self.data_df = None\n",
        "        self.protein_to_id = None\n",
        "\n",
        "        self.char_map = {\n",
        "            'A': np.array([1, 0, 0, 0], dtype=np.float32),\n",
        "            'C': np.array([0, 1, 0, 0], dtype=np.float32),\n",
        "            'G': np.array([0, 0, 1, 0], dtype=np.float32),\n",
        "            'U': np.array([0, 0, 0, 1], dtype=np.float32),\n",
        "            'N': np.array([0.25, 0.25, 0.25, 0.25], dtype=np.float32)\n",
        "        }\n",
        "        self.padding_vec = np.zeros(4, dtype=np.float32)\n",
        "\n",
        "    def _ensure_data_loaded(self):\n",
        "        if self.data_df is not None:\n",
        "            return\n",
        "\n",
        "        print(f\"Loading Metadata from {self.cfg.METADATA_PATH}...\")\n",
        "        start_time = time.time()\n",
        "        self.meta_df = pd.read_excel(\n",
        "            self.cfg.METADATA_PATH,\n",
        "            sheet_name=self.cfg.METADATA_SHEET\n",
        "        )\n",
        "        print(f\"  > Metadata loaded in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.meta_df.columns = [c.strip() for c in self.meta_df.columns]\n",
        "        self.protein_to_id = pd.Series(\n",
        "            self.meta_df['Motif_ID'].values,\n",
        "            index=self.meta_df['Protein_name']\n",
        "        ).to_dict()\n",
        "\n",
        "        print(f\"Loading Data from {self.cfg.DATA_PATH}...\")\n",
        "        start_time = time.time()\n",
        "        self.data_df = pd.read_csv(self.cfg.DATA_PATH, sep='\\t', low_memory=False)\n",
        "        print(f\"  > Data Matrix loaded in {time.time() - start_time:.2f} seconds.\")\n",
        "        self.data_df.columns = [c.strip() for c in self.data_df.columns]\n",
        "\n",
        "    def _encode_sequence(self, seq: str) -> np.ndarray:\n",
        "        if not isinstance(seq, str):\n",
        "            seq = \"N\" * self.cfg.SEQ_MAX_LEN\n",
        "        seq = seq.upper()[:self.cfg.SEQ_MAX_LEN]\n",
        "        encoded = [self.char_map.get(base, self.char_map['N']) for base in seq]\n",
        "        pad_len = self.cfg.SEQ_MAX_LEN - len(encoded)\n",
        "        if pad_len > 0:\n",
        "            encoded.extend([self.padding_vec] * pad_len)\n",
        "        return np.array(encoded, dtype=np.float32)\n",
        "\n",
        "    def _preprocess_intensities(self, intensities: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        mask = (~np.isnan(intensities)).astype(np.float32)\n",
        "        clean_vals = np.nan_to_num(intensities, nan=0.0)\n",
        "\n",
        "        if np.sum(mask) > 0:\n",
        "            valid_data = intensities[mask == 1]\n",
        "            clip_val = np.percentile(valid_data, self.cfg.CLIP_PERCENTILE)\n",
        "            clean_vals = np.clip(clean_vals, None, clip_val)\n",
        "\n",
        "        min_val = np.min(clean_vals)\n",
        "        shift = 0\n",
        "        if min_val <= 0:\n",
        "            shift = abs(min_val) + 1.0\n",
        "        clean_vals = np.log(clean_vals + shift + self.cfg.EPSILON)\n",
        "\n",
        "        masked_vals = clean_vals[mask == 1]\n",
        "        if len(masked_vals) > 0:\n",
        "            mean = np.mean(masked_vals)\n",
        "            std = np.std(masked_vals) + self.cfg.EPSILON\n",
        "            clean_vals = (clean_vals - mean) / std\n",
        "\n",
        "        clean_vals = clean_vals * mask\n",
        "        return clean_vals, mask\n",
        "\n",
        "    def get_data(self, protein_name: str, split: str = 'train') -> TensorDataset:\n",
        "        os.makedirs(self.cfg.SAVE_DIR, exist_ok=True)\n",
        "        data_path = os.path.join(self.cfg.SAVE_DIR, f\"{protein_name}_{split}_data.pt\")\n",
        "\n",
        "        if os.path.exists(data_path):\n",
        "            print(f\"Found cached data for {protein_name} ({split}). Loading...\")\n",
        "            tensors = torch.load(data_path, weights_only=True)\n",
        "            return TensorDataset(*tensors)\n",
        "\n",
        "        self._ensure_data_loaded()\n",
        "\n",
        "        if protein_name not in self.protein_to_id:\n",
        "            raise ValueError(f\"Protein '{protein_name}' not found in metadata.\")\n",
        "\n",
        "        rncmpt_id = self.protein_to_id[protein_name]\n",
        "\n",
        "        if rncmpt_id not in self.data_df.columns:\n",
        "            raise ValueError(f\"ID {rncmpt_id} for {protein_name} missing from data matrix.\")\n",
        "\n",
        "        s_lower = split.lower()\n",
        "\n",
        "        if s_lower == 'test':\n",
        "            subset = self.data_df[self.data_df['Probe_Set'] == self.cfg.TEST_SPLIT_ID].copy()\n",
        "        elif s_lower in ['train', 'val']:\n",
        "            full_set = self.data_df[self.data_df['Probe_Set'] == self.cfg.TRAIN_SPLIT_ID]\n",
        "            full_set = full_set.sort_index()\n",
        "            n_samples = len(full_set)\n",
        "            indices = np.arange(n_samples)\n",
        "            rng = np.random.RandomState(self.cfg.SEED)\n",
        "            rng.shuffle(indices)\n",
        "            val_size = int(n_samples * self.cfg.VAL_SPLIT_PCT)\n",
        "            if s_lower == 'val':\n",
        "                subset_indices = indices[:val_size]\n",
        "            else:\n",
        "                subset_indices = indices[val_size:]\n",
        "            subset = full_set.iloc[subset_indices].copy()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown split '{split}'.\")\n",
        "\n",
        "        raw_seqs = subset['RNA_Seq'].values\n",
        "        X = np.stack([self._encode_sequence(s) for s in raw_seqs])\n",
        "\n",
        "        raw_intensities = pd.to_numeric(subset[rncmpt_id], errors='coerce').values\n",
        "        Y, mask = self._preprocess_intensities(raw_intensities)\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.FloatTensor(X),\n",
        "            torch.FloatTensor(Y).unsqueeze(1),\n",
        "            torch.FloatTensor(mask).unsqueeze(1)\n",
        "        )\n",
        "\n",
        "        print(f\"Saving processed data to {data_path}...\")\n",
        "        torch.save(dataset.tensors, data_path)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "def load_rnacompete_data(protein_name: str, split: str = 'train', config: RNAConfig = None):\n",
        "    if config is None:\n",
        "        config = RNAConfig()\n",
        "    loader = RNACompeteLoader(config)\n",
        "    return loader.get_data(protein_name, split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LIA0znn3RfQ"
      },
      "source": [
        "## 5. Model Definitions\n",
        "\n",
        "### 5.1 CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG1UcssY3RfQ"
      },
      "outputs": [],
      "source": [
        "class RNABindingCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    1D Convolutional Neural Network for RNA sequence binding prediction.\n",
        "\n",
        "    Architecture:\n",
        "    - 3 convolutional layers with increasing channels (64 -> 128 -> 256)\n",
        "    - Batch normalization after each conv layer\n",
        "    - ReLU activation and dropout for regularization\n",
        "    - Global max + average pooling for richer representation\n",
        "    - 2 fully connected layers for regression output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels=4, seq_length=41, hidden_dim=128, dropout=0.3):\n",
        "        super(RNABindingCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers with different kernel sizes to capture various motif lengths\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=7, padding=3)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=9, padding=4)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Global pooling (both max and average)\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 2, hidden_dim)  # *2 for concat of max and avg pool\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: (batch, seq_length, 4)\n",
        "        # Conv1d expects: (batch, channels, seq_length)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Convolutional blocks\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Global pooling\n",
        "        max_pool = self.global_max_pool(x).squeeze(-1)\n",
        "        avg_pool = self.global_avg_pool(x).squeeze(-1)\n",
        "        x = torch.cat([max_pool, avg_pool], dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBoXj9sR3RfQ"
      },
      "source": [
        "### 5.2 LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKhMqThC3RfR"
      },
      "outputs": [],
      "source": [
        "class RNABindingLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM for RNA sequence binding prediction.\n",
        "\n",
        "    Architecture:\n",
        "    - 2-layer bidirectional LSTM\n",
        "    - Batch normalization\n",
        "    - Dropout for regularization\n",
        "    - 2 fully connected layers for regression\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=4, hidden_dim=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super(RNABindingLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim * self.num_directions)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim * self.num_directions, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: (batch, seq_length, 4)\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Concatenate last hidden states from forward and backward\n",
        "            hidden_forward = hidden[-2, :, :]\n",
        "            hidden_backward = hidden[-1, :, :]\n",
        "            combined = torch.cat([hidden_forward, hidden_backward], dim=1)\n",
        "        else:\n",
        "            combined = hidden[-1, :, :]\n",
        "\n",
        "        combined = self.bn(combined)\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        x = self.relu(self.fc1(combined))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxGTwukW3RfR"
      },
      "source": [
        "## 6. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Qx6YCy3RfR"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(x)\n",
        "        loss = masked_mse_loss(predictions, y, mask)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    \"\"\"Evaluate model and return loss and Spearman correlation.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            x, y, mask = batch\n",
        "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "\n",
        "            predictions = model(x)\n",
        "            loss = masked_mse_loss(predictions, y, mask)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            all_preds.append(predictions.cpu())\n",
        "            all_targets.append(y.cpu())\n",
        "            all_masks.append(mask.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "    all_masks = torch.cat(all_masks, dim=0)\n",
        "\n",
        "    spearman_corr = masked_spearman_correlation(all_preds, all_targets, all_masks)\n",
        "\n",
        "    return total_loss / num_batches, spearman_corr.item()\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, device,\n",
        "                num_epochs, model_name, patience=15, save_every=10):\n",
        "    \"\"\"Full training loop with early stopping and periodic checkpoints.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_correlations = []\n",
        "\n",
        "    best_val_corr = -float('inf')\n",
        "    best_model_state = None\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "        val_loss, val_corr = evaluate(model, val_loader, device)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_corr)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_correlations.append(val_corr)\n",
        "\n",
        "        if val_corr > best_val_corr:\n",
        "            best_val_corr = val_corr\n",
        "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            epochs_without_improvement = 0\n",
        "            # Save best model\n",
        "            torch.save(best_model_state, f'{model_name}_best.pth')\n",
        "            print(f\"  → Saved new best model (Spearman: {val_corr:.4f})\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} | \"\n",
        "                  f\"Val Spearman: {val_corr:.4f} | \"\n",
        "                  f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Periodic checkpoint every N epochs\n",
        "        if (epoch + 1) % save_every == 0:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'val_correlations': val_correlations,\n",
        "                'best_val_corr': best_val_corr\n",
        "            }\n",
        "            torch.save(checkpoint, f'{model_name}_checkpoint_epoch{epoch+1}.pth')\n",
        "            print(f\"  → Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
        "    print(f\"Best validation Spearman correlation: {best_val_corr:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_correlations': val_correlations,\n",
        "        'best_val_corr': best_val_corr,\n",
        "        'training_time': total_time\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvPBHoqu3RfR"
      },
      "source": [
        "## 7. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG_WFqWc3RfR",
        "outputId": "2a770bff-483f-47af-aecd-5278d05c10af"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.3\n",
        "PROTEIN_NAME = 'RBFOX1'\n",
        "\n",
        "# Load data\n",
        "print(f\"Loading data for protein: {PROTEIN_NAME}\")\n",
        "config = RNAConfig()\n",
        "\n",
        "train_dataset = load_rnacompete_data(PROTEIN_NAME, split='train', config=config)\n",
        "val_dataset = load_rnacompete_data(PROTEIN_NAME, split='val', config=config)\n",
        "test_dataset = load_rnacompete_data(PROTEIN_NAME, split='test', config=config)\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Train: {len(train_dataset)}\")\n",
        "print(f\"  Val: {len(val_dataset)}\")\n",
        "print(f\"  Test: {len(test_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XflxQ1VL3RfR"
      },
      "source": [
        "## 8. Train CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs6-d6YM3RfR",
        "outputId": "d9d2ebc5-6990-4006-a3e9-114db411898c"
      },
      "outputs": [],
      "source": [
        "configure_seed(42)\n",
        "\n",
        "cnn_model = RNABindingCNN(\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "print(\"CNN Model Architecture:\")\n",
        "print(cnn_model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
        "\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "cnn_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    cnn_optimizer, mode='max', factor=0.5, patience=7\n",
        ")\n",
        "\n",
        "cnn_history = train_model(\n",
        "    cnn_model, train_loader, val_loader, cnn_optimizer, cnn_scheduler,\n",
        "    device, num_epochs=NUM_EPOCHS, model_name='CNN', patience=15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax4WTYYe3RfR",
        "outputId": "1c92ce03-7c1c-457d-866b-29d0816cb364"
      },
      "outputs": [],
      "source": [
        "# Evaluate CNN on test set\n",
        "cnn_test_loss, cnn_test_corr = evaluate(cnn_model, test_loader, device)\n",
        "print(f\"\\nCNN Test Results:\")\n",
        "print(f\"  Test Loss: {cnn_test_loss:.4f}\")\n",
        "print(f\"  Test Spearman Correlation: {cnn_test_corr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlEimsfj3RfR"
      },
      "source": [
        "## 9. Train LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38pOFK8z3RfR",
        "outputId": "d563fe33-2603-4203-df50-756deeb2dfcb"
      },
      "outputs": [],
      "source": [
        "configure_seed(42)  # Re-seed for fair comparison\n",
        "\n",
        "lstm_model = RNABindingLSTM(\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "print(\"LSTM Model Architecture:\")\n",
        "print(lstm_model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n",
        "\n",
        "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "lstm_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    lstm_optimizer, mode='max', factor=0.5, patience=7\n",
        ")\n",
        "\n",
        "lstm_history = train_model(\n",
        "    lstm_model, train_loader, val_loader, lstm_optimizer, lstm_scheduler,\n",
        "    device, num_epochs=NUM_EPOCHS, model_name='LSTM', patience=15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKvfoFQ53RfR",
        "outputId": "9cdd7a54-131f-43c9-d0b3-5f719ce338c8"
      },
      "outputs": [],
      "source": [
        "# Evaluate LSTM on test set\n",
        "lstm_test_loss, lstm_test_corr = evaluate(lstm_model, test_loader, device)\n",
        "print(f\"\\nLSTM Test Results:\")\n",
        "print(f\"  Test Loss: {lstm_test_loss:.4f}\")\n",
        "print(f\"  Test Spearman Correlation: {lstm_test_corr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0hVYj_I3RfR"
      },
      "source": [
        "## 10. Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "pxyLw6-33RfR",
        "outputId": "b4eba921-6af7-445e-ed8f-90b24a036427"
      },
      "outputs": [],
      "source": [
        "# Comparison plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "colors = ['#2196F3', '#FF5722']  # Blue for CNN, Orange for LSTM\n",
        "\n",
        "# Plot losses\n",
        "ax1 = axes[0]\n",
        "for idx, (name, hist) in enumerate(zip(['CNN', 'LSTM'], [cnn_history, lstm_history])):\n",
        "    epochs = range(1, len(hist['train_losses']) + 1)\n",
        "    ax1.plot(epochs, hist['train_losses'], label=f'{name} - Train',\n",
        "             color=colors[idx], linestyle='-', linewidth=2)\n",
        "    ax1.plot(epochs, hist['val_losses'], label=f'{name} - Val',\n",
        "             color=colors[idx], linestyle='--', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
        "ax1.set_title('Training and Validation Loss', fontsize=14)\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot validation correlation\n",
        "ax2 = axes[1]\n",
        "for idx, (name, hist) in enumerate(zip(['CNN', 'LSTM'], [cnn_history, lstm_history])):\n",
        "    epochs = range(1, len(hist['val_correlations']) + 1)\n",
        "    ax2.plot(epochs, hist['val_correlations'], label=name,\n",
        "             color=colors[idx], linewidth=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Spearman Correlation', fontsize=12)\n",
        "ax2.set_title('Validation Spearman Correlation', fontsize=14)\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves_comparison.pdf', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "ZoIvcMSj3RfR",
        "outputId": "dc330aec-0a71-485a-84c3-ee9de16834c4"
      },
      "outputs": [],
      "source": [
        "# Individual CNN loss plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "epochs = range(1, len(cnn_history['train_losses']) + 1)\n",
        "ax.plot(epochs, cnn_history['train_losses'], label='Train Loss', color='blue', linewidth=2)\n",
        "ax.plot(epochs, cnn_history['val_losses'], label='Validation Loss', color='orange', linewidth=2)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
        "ax.set_title('CNN - Training and Validation Loss', fontsize=14)\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cnn_loss.pdf', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "8-BdVfpl3RfR",
        "outputId": "d3c631c5-2586-4b4f-ced2-de9c1cc6bf8a"
      },
      "outputs": [],
      "source": [
        "# Individual LSTM loss plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "epochs = range(1, len(lstm_history['train_losses']) + 1)\n",
        "ax.plot(epochs, lstm_history['train_losses'], label='Train Loss', color='blue', linewidth=2)\n",
        "ax.plot(epochs, lstm_history['val_losses'], label='Validation Loss', color='orange', linewidth=2)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
        "ax.set_title('LSTM - Training and Validation Loss', fontsize=14)\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('lstm_loss.pdf', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0qwr2LL3RfR"
      },
      "source": [
        "## 11. Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB4gFA8e3RfR",
        "outputId": "9a719948-1ae8-4e1b-c799-2e5c20e63548"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Model':<10} {'Val Spearman':<15} {'Test Spearman':<15} {'Test MSE':<12} {'Train Time':<12}\")\n",
        "print(\"-\" * 64)\n",
        "print(f\"{'CNN':<10} {cnn_history['best_val_corr']:<15.4f} {cnn_test_corr:<15.4f} \"\n",
        "      f\"{cnn_test_loss:<12.4f} {cnn_history['training_time']:<12.2f}s\")\n",
        "print(f\"{'LSTM':<10} {lstm_history['best_val_corr']:<15.4f} {lstm_test_corr:<15.4f} \"\n",
        "      f\"{lstm_test_loss:<12.4f} {lstm_history['training_time']:<12.2f}s\")\n",
        "\n",
        "# Save models\n",
        "torch.save(cnn_model.state_dict(), 'cnn_model.pth')\n",
        "torch.save(lstm_model.state_dict(), 'lstm_model.pth')\n",
        "print(\"\\nModels saved: cnn_model.pth, lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj4tYJya3RfR"
      },
      "source": [
        "## 12. Hyperparameter Tuning\n",
        "\n",
        "Performs grid search over hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMYloDtp3RfS"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter grid - focused on learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.001, 0.0005, 0.0001],\n",
        "    'hidden_dim': [128],  \n",
        "    'dropout': [0.3]      \n",
        "}\n",
        "\n",
        "def hyperparameter_search(model_class, model_name, param_grid, train_loader, val_loader, device, epochs=20):\n",
        "    \"\"\"Grid search for hyperparameters.\"\"\"\n",
        "    best_config = None\n",
        "    best_val_corr = -float('inf')\n",
        "    results = []\n",
        "    \n",
        "    total_configs = len(param_grid['learning_rate']) * len(param_grid['hidden_dim']) * len(param_grid['dropout'])\n",
        "    config_num = 0\n",
        "    \n",
        "    for lr in param_grid['learning_rate']:\n",
        "        for hidden_dim in param_grid['hidden_dim']:\n",
        "            for dropout in param_grid['dropout']:\n",
        "                config_num += 1\n",
        "                config = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'dropout': dropout}\n",
        "                print(f\"\\n[{config_num}/{total_configs}] Config: {config}\")\n",
        "                \n",
        "                configure_seed(42)\n",
        "                model = model_class(hidden_dim=hidden_dim, dropout=dropout).to(device)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "                \n",
        "                history = train_model(model, train_loader, val_loader, optimizer, scheduler,\n",
        "                                     device, num_epochs=epochs, model_name=model_name, patience=7)\n",
        "                \n",
        "                results.append({'config': config, 'best_val_corr': history['best_val_corr']})\n",
        "                \n",
        "                if history['best_val_corr'] > best_val_corr:\n",
        "                    best_val_corr = history['best_val_corr']\n",
        "                    best_config = config\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Best {model_name} configuration: {best_config}\")\n",
        "    print(f\"Best validation Spearman: {best_val_corr:.4f}\")\n",
        "    \n",
        "    return best_config, results\n",
        "\n",
        "# Run hyperparameter search\n",
        "best_cnn_config, cnn_tuning_results = hyperparameter_search(\n",
        "    RNABindingCNN, 'CNN', param_grid, train_loader, val_loader, device, epochs=20\n",
        ")\n",
        "\n",
        "best_lstm_config, lstm_tuning_results = hyperparameter_search(\n",
        "    RNABindingLSTM, 'LSTM', param_grid, train_loader, val_loader, device, epochs=20\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYPERPARAMETER SEARCH SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best CNN config: {best_cnn_config}\")\n",
        "print(f\"Best LSTM config: {best_lstm_config}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
