{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbXSD0GH3RfN"
   },
   "source": [
    "# Deep Learning Homework 2 - Question 2.1\n",
    "## RNA Binding Protein (RBP) Interaction Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ma8O0mu3RfO"
   },
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ao6tSGgx3RfO"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openpyxl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QjVtMaVQ3RfP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Create Output Directory\n",
    "OUTPUT_DIR = 'Outputs_bestTuning'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRH8WJjw3RfP"
   },
   "source": [
    "## 2. Download Data\n",
    "\n",
    "Download the data files from the Google Drive link provided in the homework:\n",
    "- `norm_data.txt`\n",
    "- `metadata.xlsx`\n",
    "\n",
    "Upload them to Colab or mount your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6zMi0qRK5OrF"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kC91wjUR3RfP"
   },
   "outputs": [],
   "source": [
    "# Set the path to your data files\n",
    "DATA_DIR = 'data'  # Change this to your folder path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bWdIDwo3RfP"
   },
   "source": [
    "## 3. Configuration and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dEKieqTx3RfP"
   },
   "outputs": [],
   "source": [
    "from config import RNAConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5ebeuKmQ3RfQ"
   },
   "outputs": [],
   "source": [
    "from utils import configure_seed, masked_mse_loss, masked_spearman_correlation\n",
    "\n",
    "configure_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8kY3uRe3RfQ"
   },
   "source": [
    "## 4. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n1mTI2lD3RfQ"
   },
   "outputs": [],
   "source": [
    "from utils import RNACompeteLoader, load_rnacompete_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LIA0znn3RfQ"
   },
   "source": [
    "## 5. Model Definitions\n",
    "\n",
    "### 5.1 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HG1UcssY3RfQ"
   },
   "outputs": [],
   "source": [
    "class RNABindingCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    1D Convolutional Neural Network for RNA sequence binding prediction.\n",
    "\n",
    "    Architecture:\n",
    "    - 3 convolutional layers with increasing channels (64 -> 128 -> 256)\n",
    "    - Batch normalization after each conv layer\n",
    "    - ReLU activation and dropout for regularization\n",
    "    - Global max + average pooling for richer representation\n",
    "    - 2 fully connected layers for regression output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels=4, seq_length=41, hidden_dim=128, dropout=0.3):\n",
    "        super(RNABindingCNN, self).__init__()\n",
    "        #L_out = ((L_in + 2P - K) / S)  + 1\n",
    "\n",
    "        # Convolutional layers with different kernel sizes to capture various motif lengths\n",
    "        # Padding choices allow to preserve input length through the feature maps\n",
    "        #L_out = 41\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        #L_out = 41\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=7, padding=3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        #L_out = 41\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=9, padding=4)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Global pooling (both max and average)\n",
    "        # allows to get the strongest value of a given feature for the sequence\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        # allows to get the average of how much a feature is present throughout the sequence\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 2, hidden_dim)  # *2 for concat of max and avg pool\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch, seq_length, 4)\n",
    "        # Conv1d expects: (batch, channels, seq_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        max_pool = self.global_max_pool(x).squeeze(-1)\n",
    "        avg_pool = self.global_avg_pool(x).squeeze(-1)\n",
    "        x = torch.cat([max_pool, avg_pool], dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBoXj9sR3RfQ"
   },
   "source": [
    "### 5.2 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xKhMqThC3RfR"
   },
   "outputs": [],
   "source": [
    "class RNABindingLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for RNA sequence binding prediction.\n",
    "\n",
    "    Architecture:\n",
    "    - 2-layer bidirectional LSTM\n",
    "    - Batch normalization\n",
    "    - Dropout for regularization\n",
    "    - 2 fully connected layers for regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=4, hidden_dim=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super(RNABindingLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim * self.num_directions)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim * self.num_directions, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch, seq_length, 4)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Concatenate last hidden states from forward and backward\n",
    "            hidden_forward = hidden[-2, :, :]\n",
    "            hidden_backward = hidden[-1, :, :]\n",
    "            combined = torch.cat([hidden_forward, hidden_backward], dim=1)\n",
    "        else:\n",
    "            combined = hidden[-1, :, :]\n",
    "\n",
    "        combined = self.bn(combined)\n",
    "        combined = self.dropout(combined)\n",
    "\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxGTwukW3RfR"
   },
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F3Qx6YCy3RfR"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch and return loss and correlation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_masks = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, y, mask = batch\n",
    "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = masked_mse_loss(predictions, y, mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Collect for metric calculation\n",
    "        all_preds.append(predictions.detach().cpu())\n",
    "        all_targets.append(y.detach().cpu())\n",
    "        all_masks.append(mask.detach().cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_masks = torch.cat(all_masks, dim=0)\n",
    "    \n",
    "    spearman_corr = masked_spearman_correlation(all_preds, all_targets, all_masks)\n",
    "\n",
    "    return total_loss / num_batches, spearman_corr.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluate model and return loss and Spearman correlation.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, y, mask = batch\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "            predictions = model(x)\n",
    "            loss = masked_mse_loss(predictions, y, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_targets.append(y.cpu())\n",
    "            all_masks.append(mask.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_masks = torch.cat(all_masks, dim=0)\n",
    "\n",
    "    spearman_corr = masked_spearman_correlation(all_preds, all_targets, all_masks)\n",
    "\n",
    "    return total_loss / num_batches, spearman_corr.item()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device,\n",
    "                num_epochs, model_name, patience=15, save_every=10):\n",
    "    \"\"\"Full training loop with early stopping and periodic checkpoints.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    train_losses = []\n",
    "    train_correlations = []\n",
    "    val_losses = []\n",
    "    val_correlations = []\n",
    "\n",
    "    best_val_corr = -float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        train_loss, train_corr = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_corr = evaluate(model, val_loader, device)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_corr)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_correlations.append(train_corr)\n",
    "        val_losses.append(val_loss)\n",
    "        val_correlations.append(val_corr)\n",
    "\n",
    "        if val_corr > best_val_corr:\n",
    "            best_val_corr = val_corr\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "            # Save best model to Output directory\n",
    "            torch.save(best_model_state, f'{OUTPUT_DIR}/{model_name}_best_tun.pth')\n",
    "            print(f\"  → Saved new best model (Spearman: {val_corr:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train Spearman: {train_corr:.4f} | \"\n",
    "                  f\"Val Spearman: {val_corr:.4f} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # Periodic checkpoint every N epochs\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'train_correlations': train_correlations,\n",
    "                'val_losses': val_losses,\n",
    "                'val_correlations': val_correlations,\n",
    "                'best_val_corr': best_val_corr\n",
    "            }\n",
    "            torch.save(checkpoint, f'{OUTPUT_DIR}/{model_name}_checkpoint_epoch{epoch+1}_tun.pth')\n",
    "            print(f\"  → Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
    "    print(f\"Best validation Spearman correlation: {best_val_corr:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_correlations': train_correlations,\n",
    "        'val_losses': val_losses,\n",
    "        'val_correlations': val_correlations,\n",
    "        'best_val_corr': best_val_corr,\n",
    "        'training_time': total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvPBHoqu3RfR"
   },
   "source": [
    "## 7. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LG_WFqWc3RfR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for protein: RBFOX1\n",
      "Found cached data for RBFOX1 (train). Loading from data/RBFOX1_train_data.pt...\n",
      "Found cached data for RBFOX1 (val). Loading from data/RBFOX1_val_data.pt...\n",
      "Found cached data for RBFOX1 (test). Loading from data/RBFOX1_test_data.pt...\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 96261\n",
      "  Val: 24065\n",
      "  Test: 121031\n"
     ]
    }
   ],
   "source": [
    "PROTEIN_NAME = 'RBFOX1'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading data for protein: {PROTEIN_NAME}\")\n",
    "config = RNAConfig()\n",
    "\n",
    "train_dataset = load_rnacompete_data(PROTEIN_NAME, split='train', config=config)\n",
    "val_dataset = load_rnacompete_data(PROTEIN_NAME, split='val', config=config)\n",
    "test_dataset = load_rnacompete_data(PROTEIN_NAME, split='test', config=config)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XflxQ1VL3RfR"
   },
   "source": [
    "## 8. Hyperparameter Search and Training\n",
    "\n",
    "Here we define the search space and a function to iterate through hyperparameters, training the model for each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_configs = [\n",
    "    {'learning_rate': 0.003, 'hidden_dim': 256, 'dropout': 0.2},\n",
    "    {'learning_rate': 0.001, 'hidden_dim': 256, 'dropout': 0.3},\n",
    "]\n",
    "\n",
    "lstm_configs = [\n",
    "    {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.1},\n",
    "    {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.4},\n",
    "]\n",
    "\n",
    "def run_specific_experiments(model_class, model_name, configurations, train_loader, val_loader, device, epochs=50):\n",
    "    \"\"\"\n",
    "    Runs training for a specific list of hyperparameter dictionaries.\n",
    "    Mirroring the logic of the working hyperparameter_search function.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_val_corr = -float('inf')\n",
    "    best_config = None\n",
    "    best_history = None\n",
    "    \n",
    "    print(f\"Starting Specific Experiments for {model_name} with {len(configurations)} configurations...\")\n",
    "    \n",
    "    results = []\n",
    "    # Save results to a specific JSON file to avoid overwriting grid search results\n",
    "    results_path = f'{OUTPUT_DIR}/{model_name}_specific_results.json'\n",
    "    \n",
    "    for i, config in enumerate(configurations):\n",
    "        print(f\"\\nRunning specific experiment {i+1}/{len(configurations)}: {config}\")\n",
    "        \n",
    "        configure_seed(42)\n",
    "        \n",
    "        # Initialize model with current config\n",
    "        # Handling potential init differences if any (though usually they are consistent)\n",
    "        if model_name == 'LSTM':\n",
    "             model = model_class(\n",
    "                hidden_dim=config['hidden_dim'],\n",
    "                dropout=config['dropout']\n",
    "            ).to(device)\n",
    "        else:\n",
    "             model = model_class(\n",
    "                hidden_dim=config['hidden_dim'],\n",
    "                dropout=config['dropout']\n",
    "            ).to(device)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)\n",
    "        # Using the same scheduler as your working example\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        # Train\n",
    "        # We use a unique name for temporary checkpoints to avoid conflicts\n",
    "        temp_name = f\"{model_name}_specific_exp{i}\"\n",
    "        \n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader, optimizer, scheduler,\n",
    "            device, num_epochs=epochs, model_name=temp_name, patience=7, save_every=100\n",
    "        )\n",
    "        \n",
    "        # --- SAVE PROGRESS IMMEDIATELY ---\n",
    "        run_result = {\n",
    "            'experiment_index': i,\n",
    "            'config': config,\n",
    "            'best_val_corr': history['best_val_corr'],\n",
    "            'training_time': history['training_time'],\n",
    "            'history': history \n",
    "        }\n",
    "        results.append(run_result)\n",
    "        \n",
    "        try:\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            print(f\"  -> Progress saved to {results_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Warning: Could not save progress to JSON: {e}\")\n",
    "\n",
    "        # Check if this specific run is the new best\n",
    "        if history['best_val_corr'] > best_val_corr:\n",
    "            best_val_corr = history['best_val_corr']\n",
    "            best_config = config\n",
    "            best_history = history\n",
    "            \n",
    "            # Save the best model of this specific search as the final best model\n",
    "            # We construct the path exactly as train_model is expected to save it\n",
    "            try:\n",
    "                best_state = torch.load(f'{OUTPUT_DIR}/{temp_name}_best.pth')\n",
    "                torch.save(best_state, f'{OUTPUT_DIR}/best_{model_name.lower()}_specific_model.pth')\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  -> Warning: Could not find checkpoint {temp_name}_best.pth to save as best model.\")\n",
    "            \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Best {model_name} specific configuration: {best_config}\")\n",
    "    print(f\"Best validation Spearman: {best_val_corr:.4f}\")\n",
    "    print(f\"Best model saved to {OUTPUT_DIR}/best_{model_name.lower()}_specific_model.pth\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return best_config, best_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning CNN ----\n",
      "Starting Specific Experiments for CNN with 2 configurations...\n",
      "\n",
      "Running specific experiment 1/2: {'learning_rate': 0.003, 'hidden_dim': 256, 'dropout': 0.2}\n",
      "\n",
      "============================================================\n",
      "Training CNN_specific_exp0\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.5456)\n",
      "Epoch   1/2 | Train Loss: 0.6562 | Val Loss: 0.4849 | Train Spearman: 0.4529 | Val Spearman: 0.5456 | Time: 14.86s\n",
      "  → Saved new best model (Spearman: 0.5778)\n",
      "\n",
      "Training completed in 25.78s\n",
      "Best validation Spearman correlation: 0.5778\n",
      "  -> Progress saved to Outputs_bestTuning/CNN_specific_results.json\n",
      "  -> Warning: Could not find checkpoint CNN_specific_exp0_best.pth to save as best model.\n",
      "\n",
      "Running specific experiment 2/2: {'learning_rate': 0.001, 'hidden_dim': 256, 'dropout': 0.3}\n",
      "\n",
      "============================================================\n",
      "Training CNN_specific_exp1\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.5484)\n",
      "Epoch   1/2 | Train Loss: 0.6328 | Val Loss: 0.5268 | Train Spearman: 0.4396 | Val Spearman: 0.5484 | Time: 15.02s\n",
      "  → Saved new best model (Spearman: 0.5775)\n",
      "\n",
      "Training completed in 27.46s\n",
      "Best validation Spearman correlation: 0.5775\n",
      "  -> Progress saved to Outputs_bestTuning/CNN_specific_results.json\n",
      "\n",
      "============================================================\n",
      "Best CNN specific configuration: {'learning_rate': 0.003, 'hidden_dim': 256, 'dropout': 0.2}\n",
      "Best validation Spearman: 0.5778\n",
      "Best model saved to Outputs_bestTuning/best_cnn_specific_model.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Train and Search CNN\n",
    "print(\"---- Tuning CNN ----\")\n",
    "best_cnn_config, cnn_history = run_specific_experiments(\n",
    "    RNABindingCNN, 'CNN', cnn_configs, train_loader, val_loader, device, epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning LSTM ----\n",
      "Starting Specific Experiments for LSTM with 2 configurations...\n",
      "\n",
      "Running specific experiment 1/2: {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.1}\n",
      "\n",
      "============================================================\n",
      "Training LSTM_specific_exp0\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.3076)\n",
      "Epoch   1/2 | Train Loss: 0.9641 | Val Loss: 1.3382 | Train Spearman: 0.2400 | Val Spearman: 0.3076 | Time: 41.02s\n",
      "  → Saved new best model (Spearman: 0.3744)\n",
      "\n",
      "Training completed in 69.86s\n",
      "Best validation Spearman correlation: 0.3744\n",
      "  -> Progress saved to Outputs_bestTuning/LSTM_specific_results.json\n",
      "  -> Warning: Could not find checkpoint LSTM_specific_exp0_best.pth to save as best model.\n",
      "\n",
      "Running specific experiment 2/2: {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.4}\n",
      "\n",
      "============================================================\n",
      "Training LSTM_specific_exp1\n",
      "============================================================\n",
      "  → Saved new best model (Spearman: 0.2893)\n",
      "Epoch   1/2 | Train Loss: 1.0019 | Val Loss: 0.9478 | Train Spearman: 0.1671 | Val Spearman: 0.2893 | Time: 22.79s\n",
      "  → Saved new best model (Spearman: 0.3486)\n",
      "\n",
      "Training completed in 48.66s\n",
      "Best validation Spearman correlation: 0.3486\n",
      "  -> Progress saved to Outputs_bestTuning/LSTM_specific_results.json\n",
      "\n",
      "============================================================\n",
      "Best LSTM specific configuration: {'learning_rate': 0.0001, 'hidden_dim': 256, 'dropout': 0.1}\n",
      "Best validation Spearman: 0.3744\n",
      "Best model saved to Outputs_bestTuning/best_lstm_specific_model.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8.2 Train and Search LSTM\n",
    "print(\"---- Tuning LSTM ----\")\n",
    "best_lstm_config, lstm_history = run_specific_experiments(\n",
    "    RNABindingLSTM, 'LSTM', lstm_configs, train_loader, val_loader, device, epochs=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
